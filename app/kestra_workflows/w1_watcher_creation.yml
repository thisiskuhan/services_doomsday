id: w1_watcher_creation
namespace: doomsday.watchers

description: |
  Watcher Creation Workflow (W1)
  
  EXECUTION ORDER:
  1. Clone repository and discover code entities (with unique IDs)
  2. REPO-LEVEL LLM analysis (full codebase context via Repomix)
  3. GREP + AST dependency detection (find what calls what)
  4. PER-CANDIDATE LLM analysis (with repo context + dependencies + code snippet)
  5. Store watcher and candidates with dependency signatures
  
  DB Schema Compliance:
  - Watchers: llm_business_context (TEXT), llm_tech_stack (JSONB),
              llm_architecture (TEXT), llm_health (JSONB), llm_zombie_risk (JSONB)
  - Candidates: llm_purpose (TEXT), llm_risk_score (DECIMAL 0.00-1.00), 
                llm_risk_reasoning (TEXT), depends_on_signatures (TEXT[]),
                dependency_count, caller_count

labels:
  team: doomsday
  category: watcher-management
  stage: creation
  version: "2.0"

inputs:
  - id: watcher_id
    type: STRING
    description: Unique watcher identifier (UUID from frontend)
    required: true

  - id: watcher_name
    type: STRING
    description: User-defined watcher name
    required: true

  - id: repo_url
    type: STRING
    description: GitHub repository URL
    required: true

  - id: repo_description
    type: STRING
    description: User-provided context for LLM analysis
    required: true

  - id: default_branch
    type: STRING
    description: Repository default branch
    defaults: "main"

  - id: user_id
    type: STRING
    description: User identifier from authentication
    required: true

  - id: user_email
    type: STRING
    description: User email from GitHub OAuth for notifications
    required: false

  - id: github_token
    type: STRING
    description: GitHub PAT for private repos
    required: false

  - id: application_url
    type: STRING
    description: Application base URL for endpoint monitoring
    required: false

  - id: observability_urls
    type: ARRAY
    description: Observability source URLs
    required: false
    itemType: STRING

variables:
  repo_name: "{{ inputs.repo_url | replace({'https://github.com/': '', '.git': ''}) | trim('/') }}"
  # Source: kestra_kv.yml â†’ key: w1_llm
  llm_model: "{{ kv('w1_llm') }}"

tasks:
  # ============================================================================
  # STAGE 1: Clone Repository & Discover Code Entities
  # ============================================================================
  - id: clone_and_discover
    type: io.kestra.plugin.core.flow.WorkingDirectory
    description: Clone repository and discover all code entities with unique IDs
    tasks:
      - id: clone_repo
        type: io.kestra.plugin.git.Clone
        description: Clone GitHub repository
        retry:
          type: exponential
          maxAttempts: 3
          interval: PT5S
          maxInterval: PT1M
        timeout: PT5M
        url: "{{ inputs.repo_url }}"
        username: "{{ inputs.github_token != null ? 'oauth2' : null }}"
        password: "{{ inputs.github_token }}"
        branch: "{{ inputs.default_branch }}"

      - id: discover_entities
        type: io.kestra.plugin.scripts.python.Script
        description: Scan codebase for endpoints, crons, queues with unique IDs and code snippets
        timeout: PT10M
        taskRunner:
          type: io.kestra.plugin.scripts.runner.docker.Docker
        containerImage: python:3.11-slim
        outputFiles:
          - discovery.json
          - candidates_with_code.json
        script: |
          import os
          import re
          import json
          import hashlib
          from pathlib import Path
          
          REPO_DIR = "{{ outputs.clone_repo.directory }}"
          
          def extract_nextjs_route(file_path):
              match = re.search(r'(?:src/)?app(/api/[^/]+(?:/[^/]+)*)/route\.[jt]sx?$', file_path)
              if match:
                  return match.group(1)
              match = re.search(r'(?:src/)?pages(/api/[^/]+(?:/[^/]+)*)\.[jt]sx?$', file_path)
              if match:
                  return match.group(1)
              return '/unknown'
          
          def generate_unique_id(entity_type, signature, file_path, line):
              key = f"{entity_type}:{signature}:{file_path}:{line}"
              return hashlib.md5(key.encode()).hexdigest()[:12]
          
          entities = {
              "http_endpoints": [],
              "grpc_services": [],
              "graphql_resolvers": [],
              "cron_jobs": [],
              "queue_workers": [],
              "serverless_functions": [],
              "websockets": []
          }
          
          candidates_with_code = []
          
          CODE_EXT = ('.py', '.js', '.ts', '.jsx', '.tsx', '.go', '.java', '.rb', '.php', '.rs', '.kt', '.cs')
          SKIP_DIRS = {'node_modules', 'venv', '.venv', '__pycache__', '.git', 'dist', 'build', 'vendor', '.next', 'target', 'coverage'}
          
          PATTERNS = {
              "http": [
                  (r'@(?:app|router)\.(get|post|put|delete|patch)\s*\(\s*["\']([^"\']+)', 'fastapi'),
                  (r'(?:app|router)\.(get|post|put|delete|patch)\s*\(\s*["\']([^"\']+)', 'express'),
                  (r'@app\.route\s*\(\s*["\']([^"\']+)', 'flask'),
                  (r'@(Get|Post|Put|Delete|Patch)\s*\(\s*["\']?([^"\'\)\s]*)', 'nestjs'),
                  (r'export\s+(?:async\s+)?function\s+(GET|POST|PUT|DELETE|PATCH)', 'nextjs_api'),
              ],
              "grpc": [(r'service\s+(\w+)\s*\{', 'proto')],
              "graphql": [(r'@Query\s*\(\s*\)', 'nestjs_gql'), (r'@Mutation\s*\(\s*\)', 'nestjs_gql')],
              "cron": [
                  (r'@(?:celery\.task|periodic_task|shared_task)', 'celery'),
                  (r'cron\.schedule\s*\(\s*["\']([^"\']+)', 'node_cron'),
                  (r'@Cron\s*\(\s*["\']([^"\']+)', 'nestjs_cron'),
              ],
              "queue": [
                  (r'@(?:app\.task|shared_task)', 'celery'),
                  (r'@Process\s*\(\s*["\']?([^"\']*)', 'bull'),
              ],
              "serverless": [
                  (r'exports\.handler\s*=', 'aws_lambda'),
                  (r'def\s+lambda_handler\s*\(', 'aws_lambda_py'),
              ],
              "websocket": [(r'@WebSocketGateway\s*\(\s*', 'nestjs_ws')],
          }
          
          stats = {"files_scanned": 0, "total_entities": 0}
          
          for root, dirs, files in os.walk(REPO_DIR):
              dirs[:] = [d for d in dirs if d not in SKIP_DIRS]
              
              for file in files:
                  if not file.endswith(CODE_EXT):
                      continue
                  
                  filepath = os.path.join(root, file)
                  rel_path = os.path.relpath(filepath, REPO_DIR)
                  
                  try:
                      with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                          content = f.read()
                      
                      stats["files_scanned"] += 1
                      
                      for category, patterns in PATTERNS.items():
                          for pattern, framework in patterns:
                              for match in re.finditer(pattern, content, re.MULTILINE | re.IGNORECASE):
                                  line_num = content[:match.start()].count('\n') + 1
                                  
                                  lines = content.split('\n')
                                  start_line = max(0, line_num - 10)
                                  end_line = min(len(lines), line_num + 50)
                                  code_snippet = '\n'.join(lines[start_line:end_line])
                                  
                                  entity = {
                                      "source_file": rel_path,
                                      "framework": framework,
                                      "line": line_num,
                                      "match": match.group(0)[:100]
                                  }
                                  
                                  if category == "http":
                                      groups = match.groups()
                                      method = groups[0].upper() if groups else 'GET'
                                      if framework == 'nextjs_api':
                                          path = extract_nextjs_route(rel_path)
                                      else:
                                          path = groups[1] if len(groups) > 1 else '/'
                                      entity.update({"method": method, "path": str(path)})
                                      signature = f"{method}:{path}"
                                      entities["http_endpoints"].append(entity)
                                  elif category == "grpc":
                                      entity["service_name"] = match.group(1) if match.groups() else "unknown"
                                      signature = f"grpc:{entity['service_name']}"
                                      entities["grpc_services"].append(entity)
                                  elif category == "graphql":
                                      entity["operation_type"] = "query" if "Query" in str(match.group(0)) else "mutation"
                                      signature = f"gql:{entity['operation_type']}"
                                      entities["graphql_resolvers"].append(entity)
                                  elif category == "cron":
                                      entity["schedule"] = match.group(1) if match.groups() else "scheduled"
                                      signature = f"cron:{entity.get('schedule', 'unknown')}"
                                      entities["cron_jobs"].append(entity)
                                  elif category == "queue":
                                      entity["queue_name"] = match.group(1) if match.groups() and match.group(1) else "default"
                                      signature = f"queue:{entity['queue_name']}"
                                      entities["queue_workers"].append(entity)
                                  elif category == "serverless":
                                      entity["function_name"] = match.group(1) if match.groups() and match.group(1) else rel_path
                                      signature = f"fn:{entity.get('function_name', 'handler')}"
                                      entities["serverless_functions"].append(entity)
                                  elif category == "websocket":
                                      entity["event_name"] = match.group(1) if match.groups() and match.group(1) else "connection"
                                      signature = f"ws:{entity['event_name']}"
                                      entities["websockets"].append(entity)
                                  else:
                                      signature = f"unknown:{rel_path}:{line_num}"
                                  
                                  unique_id = generate_unique_id(category, signature, rel_path, line_num)
                                  entity["unique_id"] = unique_id
                                  stats["total_entities"] += 1
                                  
                                  candidates_with_code.append({
                                      "unique_id": unique_id,
                                      "entity_type": category.rstrip('s') if category.endswith('s') else category,
                                      "signature": signature,
                                      "file_path": rel_path,
                                      "line": line_num,
                                      "framework": framework,
                                      "code_snippet": code_snippet[:3000],
                                      "entity_data": entity
                                  })
                  except Exception as e:
                      print(f"Error processing {rel_path}: {e}")
                      continue
          
          def dedupe(items):
              seen = set()
              unique = []
              for item in items:
                  key = f"{item.get('source_file')}:{item.get('line', 0)}"
                  if key not in seen:
                      seen.add(key)
                      unique.append(item)
              return unique
          
          for key in entities:
              entities[key] = dedupe(entities[key])
          
          seen_ids = set()
          unique_candidates = []
          for c in candidates_with_code:
              if c['unique_id'] not in seen_ids:
                  seen_ids.add(c['unique_id'])
                  unique_candidates.append(c)
          candidates_with_code = unique_candidates
          
          result = {
              "entities": entities,
              "summary": {
                  "files_scanned": stats["files_scanned"],
                  "http_endpoints": len(entities["http_endpoints"]),
                  "grpc_services": len(entities["grpc_services"]),
                  "graphql_resolvers": len(entities["graphql_resolvers"]),
                  "cron_jobs": len(entities["cron_jobs"]),
                  "queue_workers": len(entities["queue_workers"]),
                  "serverless_functions": len(entities["serverless_functions"]),
                  "websockets": len(entities["websockets"]),
                  "total": sum(len(v) for v in entities.values())
              }
          }
          
          with open('discovery.json', 'w') as f:
              json.dump(result, f, indent=2)
          
          with open('candidates_with_code.json', 'w') as f:
              json.dump(candidates_with_code, f, indent=2)
          
          print(f"Discovery: {stats['files_scanned']} files, {result['summary']['total']} entities")

      - id: extract_git_metadata
        type: io.kestra.plugin.scripts.shell.Commands
        description: Extract git metadata from cloned repository
        timeout: PT2M
        taskRunner:
          type: io.kestra.plugin.scripts.runner.docker.Docker
        containerImage: alpine/git:latest
        outputFiles:
          - git_metadata.json
        commands:
          - |
            cd {{ outputs.clone_repo.directory }}
            
            COMMIT_HASH=$(git rev-parse HEAD 2>/dev/null || echo "unknown")
            COMMIT_MSG=$(git log -1 --format='%s' 2>/dev/null | head -c 100 | tr '"' "'" || echo "")
            COMMIT_AUTHOR=$(git log -1 --format='%an' 2>/dev/null || echo "unknown")
            COMMIT_DATE=$(git log -1 --format='%aI' 2>/dev/null || echo "")
            
            cat > git_metadata.json << EOF
            {
              "last_commit_hash": "$COMMIT_HASH",
              "last_commit_message": "$COMMIT_MSG",
              "last_commit_author": "$COMMIT_AUTHOR",
              "last_commit_date": "$COMMIT_DATE"
            }
            EOF
            
            echo "Git metadata extracted:"
            cat git_metadata.json

      - id: pack_codebase
        type: io.kestra.plugin.scripts.shell.Commands
        description: Pack entire codebase using Repomix for LLM analysis
        timeout: PT5M
        taskRunner:
          type: io.kestra.plugin.scripts.runner.docker.Docker
        containerImage: node:20-slim
        outputFiles:
          - repomix-output.txt
        commands:
          - |
            npm install -g repomix 2>/dev/null || true
            cd {{ outputs.clone_repo.directory }}
            
            repomix --output /tmp/repomix-output.txt \
              --ignore "node_modules,dist,build,.next,coverage,*.lock,*.log" \
              --style plain . 2>/dev/null || {
              find . -type f \( -name "*.ts" -o -name "*.tsx" -o -name "*.js" -o -name "*.py" \) \
                ! -path "*/node_modules/*" ! -path "*/.git/*" \
                -exec echo "--- {} ---" \; -exec head -100 {} \; > /tmp/repomix-output.txt 2>/dev/null
            }
            
            head -c 500000 /tmp/repomix-output.txt > repomix-output.txt
            echo "Codebase packed: $(wc -c < repomix-output.txt) bytes"

  # ============================================================================
  # STAGE 2: Repository-Level LLM Analysis (RUNS FIRST!)
  # ============================================================================
  - id: llm_repo_analysis
    type: io.kestra.plugin.scripts.python.Script
    description: |
      REPO-LEVEL LLM analysis - runs FIRST to get overall context.
      Outputs: llm_business_context (TEXT), llm_tech_stack (JSONB), llm_architecture (TEXT),
      llm_health (JSONB), llm_zombie_risk (JSONB)
    timeout: PT10M
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
    containerImage: python:3.11-slim
    beforeCommands:
      - pip install -q google-genai
    env:
      REPO_DESCRIPTION: "{{ inputs.repo_description }}"
      REPO_URL: "{{ inputs.repo_url }}"
      WATCHER_NAME: "{{ inputs.watcher_name }}"
      GOOGLE_API_KEY: "{{ secret('GOOGLE_AI_API_KEY') }}"
      LLM_MODEL: "{{ vars.llm_model }}"
    inputFiles:
      discovery.json: "{{ outputs.discover_entities.outputFiles['discovery.json'] }}"
      git_metadata.json: "{{ outputs.extract_git_metadata.outputFiles['git_metadata.json'] }}"
      codebase.txt: "{{ outputs.pack_codebase.outputFiles['repomix-output.txt'] }}"
    outputFiles:
      - llm_repo_analysis.json
    script: |
      import os
      import re
      import json
      
      repo_description = os.environ.get('REPO_DESCRIPTION', '')
      repo_url = os.environ.get('REPO_URL', '')
      watcher_name = os.environ.get('WATCHER_NAME', '')
      api_key = os.environ.get('GOOGLE_API_KEY')
      llm_model = os.environ.get('LLM_MODEL')
      
      with open('discovery.json', 'r') as f:
          discovery = json.load(f)
      
      with open('git_metadata.json', 'r') as f:
          metadata = json.load(f)
      
      with open('codebase.txt', 'r', errors='ignore') as f:
          codebase = f.read()
      
      result = {
          "llm_enabled": False,
          "llm_model": llm_model,
          "llm_business_context": None,
          "llm_tech_stack": {},
          "llm_architecture": None,
          "llm_health": {},
          "llm_zombie_risk": {},
          "llm_raw_response": {},
          "error": None
      }
      
      if not api_key:
          result["error"] = "Google AI API key not configured"
      else:
          try:
              from google import genai
              client = genai.Client(api_key=api_key)
              
              summary = discovery.get("summary", {})
              
              system_prompt = """You are a senior software architect performing a comprehensive codebase audit. Your analysis will directly influence automated monitoring decisions, so precision and depth are critical.

      CRITICAL CONSTRAINTS:
      - Output MUST be valid, parseable JSON - no markdown, no explanations outside JSON
      - NEVER expose secrets, credentials, API keys, tokens, or connection strings
      - Base ALL assessments on concrete evidence from the code, not assumptions
      
      Analyze thoroughly and return this exact JSON structure:
      {
        "business_context": "Articulate the core business problem this software solves. Who are the end users? What value does it deliver? Be specific and actionable.",
        "tech_stack": {
          "languages": ["Primary and secondary languages detected"],
          "frameworks": ["All frameworks, libraries, and major dependencies"],
          "databases": ["Database systems, ORMs, caching layers"],
          "infrastructure": ["Cloud services, containerization, CI/CD, monitoring tools"]
        },
        "architecture": "Describe the architectural pattern (monolith/microservices/serverless/hybrid). Explain data flow, key components, and how they interact. Highlight any architectural concerns.",
        "health": {
          "activity_level": "active|moderate|stale|abandoned - based on commit recency, open issues, dependency freshness",
          "documentation_quality": "good|moderate|poor|none - README completeness, inline comments, API docs",
          "test_coverage": "good|moderate|poor|unknown - presence of test files, test frameworks, coverage reports",
          "code_quality": "good|moderate|poor|unknown - code organization, error handling, security practices, DRY principles"
        },
        "zombie_risk": {
          "level": "low|medium|high",
          "score": 0.0 to 1.0,
          "reasoning": "Explain your risk assessment focusing on SERVICE-LEVEL concerns: Are there API endpoints that seem unused? Cron jobs with no apparent purpose? Queue workers that might be orphaned? Serverless functions that are never invoked?",
          "high_risk_areas": ["List specific SERVICE CATEGORIES at risk - e.g., 'Authentication APIs', 'Payment Processing', 'Legacy Cron Jobs', 'Notification Workers'. DO NOT list individual files or assets - focus on business-critical service areas that may become zombies."]
        }
      }"""
              
              user_prompt = f"""Perform a deep analysis of this repository. Your findings will be used to identify zombie (unused/dead) SERVICES.

      IMPORTANT: We are monitoring SERVICES - API endpoints, cron jobs, queue workers, serverless functions. NOT individual files, assets, or static resources.

      ðŸ“¦ REPOSITORY METADATA
      â”œâ”€ Name: {watcher_name}
      â”œâ”€ URL: {repo_url}
      â””â”€ Owner's Description: {repo_description[:500]}

      ðŸ“Š GIT ACTIVITY
      â”œâ”€ Latest Commit: {metadata.get('last_commit_hash', 'unknown')[:8]}
      â”œâ”€ Author: {metadata.get('last_commit_author', 'unknown')}
      â””â”€ Date: {metadata.get('last_commit_date', 'unknown')}

      ðŸ” AUTO-DISCOVERED SERVICES (these are what we monitor for zombie status)
      â”œâ”€ HTTP API Endpoints: {summary.get('http_endpoints', 0)}
      â”œâ”€ Cron/Scheduled Jobs: {summary.get('cron_jobs', 0)}
      â”œâ”€ Queue Workers: {summary.get('queue_workers', 0)}
      â”œâ”€ Serverless Functions: {summary.get('serverless_functions', 0)}
      â””â”€ Total Monitored Services: {summary.get('total', 0)}

      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      COMPLETE CODEBASE (analyze for context about the services above)
      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      {codebase[:350000]}
      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

      Respond with ONLY the JSON object. No preamble, no explanation, no markdown formatting."""
              
              response = client.models.generate_content(
                  model=llm_model,
                  contents=[
                      {"role": "user", "parts": [{"text": system_prompt}]},
                      {"role": "model", "parts": [{"text": "I will analyze and return valid JSON only."}]},
                      {"role": "user", "parts": [{"text": user_prompt}]}
                  ]
              )
              
              if response.text:
                  json_match = re.search(r'\{[\s\S]*\}', response.text)
                  if json_match:
                      llm_json = json.loads(json_match.group())
                      result["llm_enabled"] = True
                      result["llm_business_context"] = llm_json.get("business_context", "")
                      result["llm_tech_stack"] = llm_json.get("tech_stack", {})
                      result["llm_architecture"] = llm_json.get("architecture", "")
                      result["llm_health"] = llm_json.get("health", {})
                      result["llm_zombie_risk"] = llm_json.get("zombie_risk", {})
                      result["llm_raw_response"] = llm_json
                      print(f"[OK] LLM repo analysis complete (model: {llm_model})")
          except Exception as e:
              error_msg = str(e)[:500]
              result["error"] = error_msg
              print(f"[ERROR] LLM analysis failed: {error_msg}")
              
              # FAIL the task - don't silently continue with NULL data
              import sys
              sys.exit(1)
      
      # Only write output if we have valid LLM data
      if not result["llm_enabled"]:
          print(f"[ERROR] LLM analysis did not produce valid results")
          if result["error"]:
              print(f"[ERROR] Reason: {result['error']}")
          import sys
          sys.exit(1)
      
      with open('llm_repo_analysis.json', 'w') as f:
          json.dump(result, f, indent=2)

  # ============================================================================
  # STAGE 3: GREP + AST Dependency Detection
  # ============================================================================
  - id: detect_dependencies
    type: io.kestra.plugin.scripts.python.Script
    description: |
      Advanced dependency detection using multiple strategies:
      1. Direct path matching (strings, template literals)
      2. HTTP client patterns (fetch, axios, got, request, ky, ofetch)
      3. Route-based detection (Next.js API routes, Express patterns)
      4. Import/require analysis for internal modules
      5. Function call tracking between candidates
    timeout: PT15M
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
    containerImage: python:3.11-slim
    env:
      REPO_DIR: "{{ outputs.clone_repo.directory }}"
    inputFiles:
      candidates_with_code.json: "{{ outputs.discover_entities.outputFiles['candidates_with_code.json'] }}"
    outputFiles:
      - dependencies.json
    script: |
      import os
      import re
      import json
      from pathlib import Path
      
      # Escape braces for Kestra Pebble template engine
      OPEN_BRACE = "{"
      CLOSE_BRACE = "}"
      
      REPO_DIR = os.environ.get('REPO_DIR', '.')
      
      with open('candidates_with_code.json', 'r') as f:
          candidates = json.load(f)
      
      CODE_EXT = ('.py', '.js', '.ts', '.jsx', '.tsx', '.go', '.java', '.mjs', '.cjs')
      SKIP_DIRS = {'node_modules', 'venv', '.venv', '__pycache__', '.git', 'dist', 'build', '.next', 'coverage', '.turbo'}
      
      # Read all code files
      all_files = {}
      for root, dirs, files in os.walk(REPO_DIR):
          dirs[:] = [d for d in dirs if d not in SKIP_DIRS]
          for file in files:
              if file.endswith(CODE_EXT):
                  filepath = os.path.join(root, file)
                  rel_path = os.path.relpath(filepath, REPO_DIR)
                  try:
                      with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                          all_files[rel_path] = f.read()
                  except:
                      pass
      
      # Map files to candidates
      file_to_candidates = {}
      for c in candidates:
          fp = c['file_path']
          if fp not in file_to_candidates:
              file_to_candidates[fp] = []
          file_to_candidates[fp].append(c)
      
      sig_to_id = {c['signature']: c['unique_id'] for c in candidates}
      
      def build_http_patterns(path):
          """Build comprehensive patterns for HTTP endpoint calls."""
          if not path or path == '/unknown':
              return []
          
          patterns = []
          
          # Extract key path segments for flexible matching
          # /api/currency/[country] -> ['api', 'currency']
          segments = [s for s in path.split('/') if s and not s.startswith('[') and not s.startswith(':') and not s.startswith('{')]
          
          # Get the distinctive segment (usually the resource name like 'currency', 'animal', 'capital')
          key_segment = segments[-1] if segments else ''
          
          # Get path without dynamic segments for exact matching
          # /api/currency/[country] -> /api/currency
          static_path = '/'.join([''] + [s for s in path.split('/') if s and not s.startswith('[') and not s.startswith(':') and not s.startswith('{')])
          static_path_escaped = re.escape(static_path)
          
          print(f"  Building patterns for: {path}")
          print(f"    key_segment: {key_segment}")
          print(f"    static_path: {static_path}")
          
          # PATTERN GROUP 1: Match the key segment in any URL context (ALL LANGUAGES)
          # This catches /api/currency, /currency, currency in URLs
          if key_segment and len(key_segment) > 2:
              patterns.extend([
                  # In JS/TS template literals: `.../${baseUrl}/api/currency/${country}...`
                  rf'`[^`]*/{key_segment}[^`]*`',
                  
                  # In string literals (all languages) - match /key_segment anywhere
                  rf'["\'][^"\']*/{key_segment}[^"\']*["\']',
              ])
          
          # PATTERN GROUP 2: Match static path prefix (ALL LANGUAGES)
          if static_path and static_path != '/':
              patterns.extend([
                  # JS/TS template literals
                  rf'`[^`]*{static_path_escaped}[^`]*`',
                  
                  # Universal string literals (all languages)
                  rf'["\'][^"\']*{static_path_escaped}[^"\']*["\']',
                  
                  # JavaScript/TypeScript: fetch/axios
                  rf'fetch\s*\([^)]*{static_path_escaped}',
                  rf'axios[.\w]*\([^)]*{static_path_escaped}',
                  
                  # Python: requests, httpx, aiohttp, urllib
                  rf'requests\.\w+\s*\([^)]*{static_path_escaped}',
                  rf'httpx\.\w+\s*\([^)]*{static_path_escaped}',
                  rf'aiohttp\.\w+\s*\([^)]*{static_path_escaped}',
                  rf'urlopen\s*\([^)]*{static_path_escaped}',
                  rf'urllib\.\w+\.\w+\s*\([^)]*{static_path_escaped}',
                  
                  # Go: http.Get, http.Post, http.NewRequest
                  rf'http\.\w+\s*\([^)]*{static_path_escaped}',
                  rf'http\.NewRequest\s*\([^)]*{static_path_escaped}',
                  
                  # Java: HttpClient, RestTemplate, WebClient
                  rf'HttpClient[^)]*{static_path_escaped}',
                  rf'RestTemplate[^)]*{static_path_escaped}',
                  rf'WebClient[^)]*{static_path_escaped}',
                  rf'\.uri\s*\([^)]*{static_path_escaped}',
                  rf'\.url\s*\([^)]*{static_path_escaped}',
                  
                  # Ruby: Net::HTTP, HTTParty, Faraday
                  rf'Net::HTTP[^)]*{static_path_escaped}',
                  rf'HTTParty\.\w+\s*\([^)]*{static_path_escaped}',
                  rf'Faraday\.\w+\s*\([^)]*{static_path_escaped}',
                  
                  # PHP: curl, Guzzle, file_get_contents
                  rf'curl_\w+\s*\([^)]*{static_path_escaped}',
                  rf'Guzzle[^)]*{static_path_escaped}',
                  rf'file_get_contents\s*\([^)]*{static_path_escaped}',
                  
                  # C#/.NET: HttpClient
                  rf'HttpClient[^)]*{static_path_escaped}',
                  rf'\.GetAsync\s*\([^)]*{static_path_escaped}',
                  rf'\.PostAsync\s*\([^)]*{static_path_escaped}',
              ])
          
          # PATTERN GROUP 3: API endpoint references in config objects
          if key_segment:
              patterns.extend([
                  # { endpoint: '/api/currency' } or routes: { currency: '...' }
                  rf'{key_segment}\s*[=:]\s*[`"\'][^`"\']*{static_path_escaped}' if static_path else None,
                  
                  # apiEndpoints.currency or routes['currency']
                  rf'\.{key_segment}\b' if len(key_segment) > 3 else None,
                  rf'\[[\'"]{key_segment}[\'"]\]',
              ])
          
          # PATTERN GROUP 4: Internal API calls / String interpolation (multiple languages)
          if static_path:
              patterns.extend([
                  # JS/TS: `${origin}/api/currency...`
                  rf'\$\{{[^}}]*\}}{static_path_escaped}',
                  # String concatenation: baseUrl + '/api/currency'
                  rf'\+\s*[`"\'][^`"\']*{static_path_escaped}',
                  # Python f-strings: f"{base_url}/api/currency"
                  rf'\{{[^}}]*\}}{static_path_escaped}',
                  # Python .format(): "{}/api/currency".format(...)
                  rf'\.format\s*\([^)]*\)[^"\']*{static_path_escaped}',
              ])
          
          # PATTERN GROUP 5: Route/path declarations (ALL LANGUAGES)
          patterns.extend([
              # path: '/api/currency' or route: '/api/currency'
              rf'(?:path|route|url|endpoint|uri)\s*[=:]\s*[`"\'][^`"\']*{static_path_escaped}' if static_path else None,
          ])
          
          # Filter out None patterns
          valid_patterns = [p for p in patterns if p]
          print(f"    Generated {len(valid_patterns)} patterns")
          
          return valid_patterns
      
      def build_function_patterns(snippet, entity_name):
          """Build patterns for function/method calls."""
          patterns = []
          
          # Extract function name from code snippet
          func_patterns = [
              r'(?:def|function|const|let|var|async function|async)\s+(\w+)',
              r'export\s+(?:default\s+)?(?:async\s+)?function\s+(\w+)',
              r'export\s+const\s+(\w+)\s*=',
              r'module\.exports\s*=\s*(?:async\s+)?function\s+(\w+)',
              r'class\s+(\w+)',
          ]
          
          func_names = set()
          for fp in func_patterns:
              for match in re.finditer(fp, snippet):
                  func_names.add(match.group(1))
          
          # Also use entity_name if it looks like a function name
          if entity_name and re.match(r'^[a-zA-Z_]\w*$', entity_name):
              func_names.add(entity_name)
          
          for func_name in func_names:
              if len(func_name) > 2:  # Skip very short names
                  patterns.extend([
                      rf'\b{func_name}\s*\(',           # Direct call
                      rf'\.{func_name}\s*\(',           # Method call
                      rf'await\s+{func_name}\s*\(',     # Async call
                      rf'{func_name}\s*\.\s*call\s*\(', # .call()
                      rf'{func_name}\s*\.\s*apply\s*\(', # .apply()
                  ])
          
          return patterns
      
      def build_queue_patterns(queue_name):
          """Build patterns for queue/message calls."""
          if not queue_name:
              return []
          
          queue_escaped = re.escape(queue_name)
          patterns = [
              rf'["\']({queue_escaped})["\']',
              rf'queue\s*\(\s*["\']({queue_escaped})["\']',
              rf'publish\s*\(\s*["\']({queue_escaped})["\']',
              rf'send\s*\(\s*["\']({queue_escaped})["\']',
              rf'emit\s*\(\s*["\']({queue_escaped})["\']',
              rf'dispatch\s*\(\s*["\']({queue_escaped})["\']',
              rf'channel\s*\.\s*\w+\s*\(\s*["\']({queue_escaped})["\']',
              rf'producer\s*\.\s*\w+\s*\(\s*["\']({queue_escaped})["\']',
          ]
          return patterns
      
      # Build patterns for each candidate
      sig_to_patterns = {}
      for candidate in candidates:
          sig = candidate['signature']
          entity_type = candidate['entity_type']
          patterns = []
          
          if entity_type == 'http_endpoint':
              entity_data = candidate.get('entity_data', {})
              path = entity_data.get('path', '')
              patterns.extend(build_http_patterns(path))
          
          elif entity_type in ('cron_job', 'serverless_function', 'scheduled_task'):
              snippet = candidate.get('code_snippet', '')
              entity_name = candidate.get('entity_name', '')
              patterns.extend(build_function_patterns(snippet, entity_name))
          
          elif entity_type in ('queue_worker', 'message_handler', 'event_handler'):
              entity_data = candidate.get('entity_data', {})
              queue_name = entity_data.get('queue_name', '') or entity_data.get('event_name', '')
              patterns.extend(build_queue_patterns(queue_name))
              # Also add function patterns for the handler
              snippet = candidate.get('code_snippet', '')
              entity_name = candidate.get('entity_name', '')
              patterns.extend(build_function_patterns(snippet, entity_name))
          
          elif entity_type in ('grpc_service', 'graphql_resolver', 'websocket_handler'):
              snippet = candidate.get('code_snippet', '')
              entity_name = candidate.get('entity_name', '')
              patterns.extend(build_function_patterns(snippet, entity_name))
          
          if patterns:
              sig_to_patterns[sig] = patterns
      
      # Initialize dependency tracking
      dependencies = {}
      for candidate in candidates:
          sig = candidate['signature']
          dependencies[sig] = {
              "unique_id": candidate['unique_id'],
              "signature": sig,
              "entity_type": candidate['entity_type'],
              "file_path": candidate['file_path'],
              "callers": [],
              "called_by_signatures": [],
              "caller_count": 0,
              "depends_on_signatures": [],
              "dependency_count": 0,
              "is_potentially_unused": True
          }
      
      # Scan all files for dependency relationships
      print(f"\nScanning {len(all_files)} files for dependencies...")
      print(f"Looking for patterns in {len(sig_to_patterns)} candidates")
      
      matches_found = 0
      for rel_path, content in all_files.items():
          candidates_in_file = file_to_candidates.get(rel_path, [])
          candidate_sigs_in_file = {c['signature'] for c in candidates_in_file}
          
          for target_sig, patterns in sig_to_patterns.items():
              # Skip self-references (don't look for calls to an endpoint IN the same file that defines it)
              if target_sig in candidate_sigs_in_file:
                  continue
              
              for pattern in patterns:
                  try:
                      for match in re.finditer(pattern, content, re.IGNORECASE | re.MULTILINE):
                          matches_found += 1
                          line_num = content[:match.start()].count('\n') + 1
                          lines = content.split('\n')
                          start = max(0, line_num - 2)
                          end = min(len(lines), line_num + 2)
                          context = '\n'.join(lines[start:end])
                          
                          print(f"  MATCH FOUND: {target_sig}")
                          print(f"    File: {rel_path}:{line_num}")
                          print(f"    Pattern: {pattern[:60]}...")
                          print(f"    Match: {match.group(0)[:80]}")
                          
                          caller_info = {
                              "file": rel_path,
                              "line": line_num,
                              "match": match.group(0)[:100],
                              "context": context[:300],
                              "pattern_matched": pattern[:50]
                          }
                          
                          # Add to target's callers
                          existing_callers = [f"{c['file']}:{c['line']}" for c in dependencies[target_sig]["callers"]]
                          caller_key = f"{rel_path}:{line_num}"
                          if caller_key not in existing_callers:
                              dependencies[target_sig]["callers"].append(caller_info)
                          
                          # Link to calling candidate if in same file
                          for caller_candidate in candidates_in_file:
                              caller_line = caller_candidate.get('line', 0)
                              # Expand range to 100 lines for better matching in larger files
                              if abs(line_num - caller_line) < 100:
                                  caller_sig = caller_candidate['signature']
                                  if target_sig not in dependencies[caller_sig]["depends_on_signatures"]:
                                      dependencies[caller_sig]["depends_on_signatures"].append(target_sig)
                                  if caller_sig not in dependencies[target_sig]["called_by_signatures"]:
                                      dependencies[target_sig]["called_by_signatures"].append(caller_sig)
                  except re.error as e:
                      print(f"  REGEX ERROR: {pattern[:50]}... -> {e}")
      
      print(f"\nTotal matches found: {matches_found}")
      
      # Finalize counts
      for sig in dependencies:
          dependencies[sig]["caller_count"] = len(dependencies[sig]["callers"])
          dependencies[sig]["dependency_count"] = len(dependencies[sig]["depends_on_signatures"])
          dependencies[sig]["callers"] = dependencies[sig]["callers"][:20]
          dependencies[sig]["is_potentially_unused"] = dependencies[sig]["caller_count"] == 0
      
      with open('dependencies.json', 'w') as f:
          json.dump(dependencies, f, indent=2)
      
      # Print detailed summary
      deps_found = sum(1 for d in dependencies.values() if d['dependency_count'] > 0)
      callers_found = sum(1 for d in dependencies.values() if d['caller_count'] > 0)
      unused = sum(1 for d in dependencies.values() if d['is_potentially_unused'])
      
      print(f"\n{'='*60}")
      print(f"DEPENDENCY ANALYSIS COMPLETE")
      print(f"{'='*60}")
      print(f"Total candidates analyzed: {len(dependencies)}")
      print(f"Candidates with dependencies (call others): {deps_found}")
      print(f"Candidates with callers (called by others): {callers_found}")
      print(f"Potentially unused (no callers): {unused}")
      print(f"{'='*60}")
      
      # Show specific relationships found
      if deps_found > 0:
          print(f"\nDependency relationships found:")
          for sig, data in dependencies.items():
              if data['dependency_count'] > 0:
                  print(f"  {sig}")
                  print(f"    -> depends on: {data['depends_on_signatures']}")
      
      if callers_found > 0:
          print(f"\nCaller relationships found:")
          for sig, data in dependencies.items():
              if data['caller_count'] > 0:
                  print(f"  {sig}")
                  print(f"    <- called by: {data['called_by_signatures'][:5]}")
                  print(f"    <- from files: {[c['file'] for c in data['callers'][:3]]}")

  # ============================================================================
  # STAGE 4: Per-Candidate LLM Analysis
  # ============================================================================
  - id: llm_candidate_analysis
    type: io.kestra.plugin.scripts.python.Script
    description: |
      Analyze each candidate with LLM using:
      - Repo-level context (from Stage 2)
      - Dependencies info (from Stage 3)
      - Code snippet (from Stage 1)
    timeout: PT30M
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
    containerImage: python:3.11-slim
    beforeCommands:
      - pip install -q google-genai
    env:
      GOOGLE_API_KEY: "{{ secret('GOOGLE_AI_API_KEY') }}"
      REPO_DESCRIPTION: "{{ inputs.repo_description }}"
      LLM_MODEL: "{{ vars.llm_model }}"
    inputFiles:
      candidates_with_code.json: "{{ outputs.discover_entities.outputFiles['candidates_with_code.json'] }}"
      dependencies.json: "{{ outputs.detect_dependencies.outputFiles['dependencies.json'] }}"
      llm_repo_analysis.json: "{{ outputs.llm_repo_analysis.outputFiles['llm_repo_analysis.json'] }}"
    outputFiles:
      - candidate_analysis.json
    script: |
      import os
      import re
      import json
      import time
      
      # Escape braces for JSON template in prompt (Kestra Pebble conflict)
      open_brace = "{"
      close_brace = "}"
      
      api_key = os.environ.get('GOOGLE_API_KEY')
      repo_context = os.environ.get('REPO_DESCRIPTION', '')
      llm_model = os.environ.get('LLM_MODEL')
      
      with open('candidates_with_code.json', 'r') as f:
          candidates = json.load(f)
      
      with open('dependencies.json', 'r') as f:
          dependencies = json.load(f)
      
      with open('llm_repo_analysis.json', 'r') as f:
          repo_analysis = json.load(f)
      
      repo_business = repo_analysis.get('llm_business_context', '') or ''
      repo_tech = repo_analysis.get('llm_tech_stack', {})
      repo_arch = repo_analysis.get('llm_architecture', '') or ''
      
      results = []
      
      if not api_key:
          print("[FATAL] No GOOGLE_API_KEY configured - cannot perform LLM analysis")
          import sys
          sys.exit(1)
      
      from google import genai
      client = genai.Client(api_key=api_key)
      
      for i, candidate in enumerate(candidates):
          sig = candidate['signature']
          deps = dependencies.get(sig, {})
          caller_count = deps.get('caller_count', 0)
          callers_info = deps.get('callers', [])[:5]
          depends_on = deps.get('depends_on_signatures', [])
          
          try:
              prompt = f"""You are a zombie SERVICE detective. Your mission: determine if this SERVICE/ENDPOINT is actively used or potentially dead/unused.

      IMPORTANT: We are analyzing SERVICES (API endpoints, cron jobs, queue workers, serverless functions) - NOT files, assets, or static resources.

      ðŸ¢ REPOSITORY CONTEXT
      â”œâ”€ Business Purpose: {repo_business[:300]}
      â”œâ”€ Tech Stack: {json.dumps(repo_tech)[:200]}
      â””â”€ Architecture: {repo_arch[:200]}

      ðŸŽ¯ TARGET SERVICE
      â”œâ”€ Service Type: {candidate['entity_type']}
      â”œâ”€ Signature: {sig}
      â”œâ”€ Location: {candidate['file_path']}
      â””â”€ Framework: {candidate['framework']}

      ðŸ“ SERVICE CODE
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      {candidate['code_snippet'][:2000]}
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

      ðŸ”— DEPENDENCY ANALYSIS (static analysis results)
      â”œâ”€ Times called by other code: {caller_count}
      â”œâ”€ Flagged as potentially unused: {deps.get('is_potentially_unused', 'unknown')}
      â”œâ”€ Dependencies it relies on: {depends_on[:5] if depends_on else 'None detected'}
      â””â”€ Sample callers found: {json.dumps(callers_info, indent=2)[:400] if callers_info else 'No internal callers found'}

      âš ï¸ ZOMBIE SERVICE DETECTION CRITERIA
      Consider these signals when scoring:
      â€¢ caller_count=0 is a signal, but external APIs (called by browsers/clients/mobile apps) legitimately have 0 internal callers
      â€¢ Look for: deprecated comments, TODO remove, dead code patterns, unused exports
      â€¢ Business-critical services (auth, payments, user management) should have LOW risk even if caller_count=0
      â€¢ Generic utility endpoints or test routes with no clear purpose are HIGH risk
      â€¢ Cron jobs without clear scheduling purpose are HIGH risk
      â€¢ Queue workers processing unknown/undefined queues are HIGH risk

      Return ONLY valid JSON (no markdown, no explanation):
      {open_brace}
        "purpose": "Describe what this SERVICE does and its business value. Be specific - what user problem does it solve?",
        "risk_score": 0.00 to 1.00,
        "risk_reasoning": "Justify your score. Reference specific evidence: caller count, code patterns, business criticality, naming conventions, comments in code."
      {close_brace}

      SCORING GUIDE:
      â€¢ 0.00-0.20: Critical service (auth, payments, core business logic, essential APIs)
      â€¢ 0.21-0.40: Active service (useful utility, clearly called externally or internally)
      â€¢ 0.41-0.60: Uncertain (no clear evidence either way, needs observation)
      â€¢ 0.61-0.80: Possibly zombie (no callers, generic code, unclear business value)
      â€¢ 0.81-1.00: Likely zombie (deprecated markers, dead code patterns, orphaned service)

      JSON:"""
              
              response = client.models.generate_content(
                  model=llm_model,
                  contents=prompt
              )
              
              if response.text:
                  text = response.text.strip()
                  
                  # Remove markdown code blocks if present
                  text = re.sub(r'^```(?:json)?\s*', '', text)
                  text = re.sub(r'\s*```$', '', text)
                  text = text.strip()
                  
                  # Try to find JSON object
                  json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', text)
                  if json_match:
                      try:
                          llm_json = json.loads(json_match.group())
                          
                          risk_score = llm_json.get('risk_score', 0.5)
                          if isinstance(risk_score, (int, float)):
                              risk_score = max(0.0, min(1.0, float(risk_score)))
                          else:
                              risk_score = 0.5
                          
                          results.append({
                              "signature": sig,
                              "unique_id": candidate['unique_id'],
                              "llm_purpose": llm_json.get('purpose', '')[:2000],
                              "llm_risk_score": round(risk_score, 2),
                              "llm_risk_reasoning": llm_json.get('risk_reasoning', '')[:2000],
                              "depends_on_signatures": depends_on,
                              "error": None
                          })
                          print(f"[{i+1}/{len(candidates)}] {sig}: risk={risk_score:.2f}")
                      except json.JSONDecodeError as je:
                          results.append({
                              "signature": sig,
                              "unique_id": candidate['unique_id'],
                              "llm_purpose": None,
                              "llm_risk_score": 0.5,
                              "llm_risk_reasoning": f"JSON decode error: {str(je)[:100]}",
                              "depends_on_signatures": depends_on,
                              "error": "JSON parse failed"
                          })
                          print(f"[{i+1}/{len(candidates)}] {sig}: JSON decode failed")
                  else:
                      results.append({
                          "signature": sig,
                          "unique_id": candidate['unique_id'],
                          "llm_purpose": None,
                          "llm_risk_score": 0.5,
                          "llm_risk_reasoning": "No JSON found in LLM response",
                          "depends_on_signatures": depends_on,
                          "error": "No JSON in response"
                      })
                      print(f"[{i+1}/{len(candidates)}] {sig}: No JSON found")
              else:
                  results.append({
                      "signature": sig,
                      "unique_id": candidate['unique_id'],
                      "llm_purpose": None,
                      "llm_risk_score": 0.5,
                      "llm_risk_reasoning": None,
                      "depends_on_signatures": depends_on,
                      "error": "Empty response"
                  })
              
              time.sleep(0.5)
              
          except Exception as e:
              results.append({
                  "signature": sig,
                  "unique_id": candidate['unique_id'],
                  "llm_purpose": None,
                  "llm_risk_score": 0.5,
                  "llm_risk_reasoning": None,
                  "depends_on_signatures": depends_on,
                  "error": str(e)[:200]
              })
              print(f"[ERROR] {sig}: {e}")
              
              # Check for rate limit errors - fail immediately if quota exhausted
              error_str = str(e).lower()
              if '429' in error_str or 'quota' in error_str or 'rate' in error_str:
                  print(f"[FATAL] Rate limit or quota exceeded. Failing task.")
                  import sys
                  sys.exit(1)
              
              time.sleep(1)
      
      # Check if all candidates failed - that's a problem
      successful = [r for r in results if r.get('llm_purpose') is not None]
      failed = [r for r in results if r.get('error') is not None]
      
      if len(results) > 0 and len(successful) == 0:
          print(f"[FATAL] All {len(failed)} candidates failed LLM analysis")
          import sys
          sys.exit(1)
      
      with open('candidate_analysis.json', 'w') as f:
          json.dump(results, f, indent=2)
      
      print(f"[OK] Candidate analysis complete: {len(successful)}/{len(results)} successful")

  # ============================================================================
  # STAGE 5: Store Watcher in Database
  # ============================================================================
  - id: store_watcher
    type: io.kestra.plugin.scripts.python.Script
    description: Store watcher with LLM analysis in PostgreSQL
    retry:
      type: exponential
      maxAttempts: 3
      interval: PT2S
      maxInterval: PT30S
    timeout: PT3M
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
    containerImage: python:3.11-slim
    beforeCommands:
      - pip install -q psycopg2-binary
    env:
      DATABASE_URL: "{{ secret('DATABASE_URL') }}"
      WATCHER_ID: "{{ inputs.watcher_id }}"
      WATCHER_NAME: "{{ inputs.watcher_name }}"
      USER_ID: "{{ inputs.user_id }}"
      USER_EMAIL: "{{ inputs.user_email ?? '' }}"
      REPO_URL: "{{ inputs.repo_url }}"
      REPO_NAME: "{{ vars.repo_name }}"
      REPO_DESCRIPTION: "{{ inputs.repo_description }}"
      DEFAULT_BRANCH: "{{ inputs.default_branch }}"
      APPLICATION_URL: "{{ inputs.application_url ?? '' }}"
      OBSERVABILITY_URLS: "{{ inputs.observability_urls ?? '[]' }}"
    inputFiles:
      discovery.json: "{{ outputs.discover_entities.outputFiles['discovery.json'] }}"
      git_metadata.json: "{{ outputs.extract_git_metadata.outputFiles['git_metadata.json'] }}"
      llm_repo_analysis.json: "{{ outputs.llm_repo_analysis.outputFiles['llm_repo_analysis.json'] }}"
    outputFiles:
      - storage_result.json
    script: |
      import json
      import os
      import psycopg2
      
      db_url = os.environ.get('DATABASE_URL')
      
      with open('discovery.json', 'r') as f:
          discovery = json.load(f)
      
      with open('git_metadata.json', 'r') as f:
          metadata = json.load(f)
      
      with open('llm_repo_analysis.json', 'r') as f:
          llm = json.load(f)
      
      summary = discovery.get('summary', {})
      
      conn = psycopg2.connect(db_url, sslmode='require')
      cur = conn.cursor()
      
      obs_urls = os.environ.get('OBSERVABILITY_URLS', '[]')
      try:
          obs_urls = json.loads(obs_urls) if obs_urls else []
      except:
          obs_urls = []
      
      last_date = metadata.get('last_commit_date', '').strip()
      if last_date:
          try:
              from datetime import datetime
              last_date = datetime.fromisoformat(last_date.replace('Z', '+00:00'))
          except:
              last_date = None
      else:
          last_date = None
      
      cur.execute("""
          INSERT INTO watchers (
              watcher_id, watcher_name, user_id, user_email, repo_url, repo_name, repo_description,
              default_branch, application_url, observability_urls,
              total_candidates, http_endpoints, cron_jobs, queue_workers,
              serverless_functions, websockets, grpc_services, graphql_resolvers,
              last_commit_hash, last_commit_message, last_commit_author, last_commit_date,
              llm_business_context, llm_tech_stack, llm_architecture, llm_health,
              llm_zombie_risk, llm_raw_response, status
          ) VALUES (
              %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,
              %s, %s, %s, %s, %s, %s, %s, 'pending_schedule'
          )
          ON CONFLICT (watcher_id) DO UPDATE SET
              watcher_name = EXCLUDED.watcher_name,
              user_email = COALESCE(EXCLUDED.user_email, watchers.user_email),
              total_candidates = EXCLUDED.total_candidates,
              http_endpoints = EXCLUDED.http_endpoints,
              cron_jobs = EXCLUDED.cron_jobs,
              queue_workers = EXCLUDED.queue_workers,
              serverless_functions = EXCLUDED.serverless_functions,
              websockets = EXCLUDED.websockets,
              grpc_services = EXCLUDED.grpc_services,
              graphql_resolvers = EXCLUDED.graphql_resolvers,
              last_commit_hash = EXCLUDED.last_commit_hash,
              last_commit_message = EXCLUDED.last_commit_message,
              last_commit_author = EXCLUDED.last_commit_author,
              last_commit_date = EXCLUDED.last_commit_date,
              llm_business_context = EXCLUDED.llm_business_context,
              llm_tech_stack = EXCLUDED.llm_tech_stack,
              llm_architecture = EXCLUDED.llm_architecture,
              llm_health = EXCLUDED.llm_health,
              llm_zombie_risk = EXCLUDED.llm_zombie_risk,
              llm_raw_response = EXCLUDED.llm_raw_response,
              scan_count = watchers.scan_count + 1,
              updated_at = NOW()
      """, (
          os.environ.get('WATCHER_ID'),
          os.environ.get('WATCHER_NAME'),
          os.environ.get('USER_ID'),
          os.environ.get('USER_EMAIL') or None,
          os.environ.get('REPO_URL'),
          os.environ.get('REPO_NAME'),
          os.environ.get('REPO_DESCRIPTION', '')[:500],
          os.environ.get('DEFAULT_BRANCH', 'main'),
          os.environ.get('APPLICATION_URL') or None,
          json.dumps(obs_urls),
          summary.get('total', 0),
          summary.get('http_endpoints', 0),
          summary.get('cron_jobs', 0),
          summary.get('queue_workers', 0),
          summary.get('serverless_functions', 0),
          summary.get('websockets', 0),
          summary.get('grpc_services', 0),
          summary.get('graphql_resolvers', 0),
          metadata.get('last_commit_hash'),
          metadata.get('last_commit_message', '')[:100],
          metadata.get('last_commit_author'),
          last_date,
          llm.get('llm_business_context')[:2000] if llm.get('llm_business_context') else None,
          json.dumps(llm.get('llm_tech_stack', {})),
          llm.get('llm_architecture')[:2000] if llm.get('llm_architecture') else None,
          json.dumps(llm.get('llm_health', {})),
          json.dumps(llm.get('llm_zombie_risk', {})),
          json.dumps(llm.get('llm_raw_response', {}))
      ))
      
      conn.commit()
      cur.close()
      conn.close()
      
      result = {
          "watcher_id": os.environ.get('WATCHER_ID'),
          "status": "success",
          "total_candidates": summary.get('total', 0),
          "llm_enabled": llm.get('llm_enabled', False)
      }
      
      with open('storage_result.json', 'w') as f:
          json.dump(result, f, indent=2)
      
      print(f"Watcher stored: {result['watcher_id']}")

  # ============================================================================
  # STAGE 6: Store Candidates with Dependencies
  # ============================================================================
  - id: store_candidates
    type: io.kestra.plugin.scripts.python.Script
    description: Store zombie candidates with LLM analysis and dependency signatures
    retry:
      type: exponential
      maxAttempts: 3
      interval: PT2S
      maxInterval: PT30S
    timeout: PT5M
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
    containerImage: python:3.11-slim
    beforeCommands:
      - pip install -q psycopg2-binary
    env:
      DATABASE_URL: "{{ secret('DATABASE_URL') }}"
      WATCHER_ID: "{{ inputs.watcher_id }}"
    inputFiles:
      discovery.json: "{{ outputs.discover_entities.outputFiles['discovery.json'] }}"
      candidate_analysis.json: "{{ outputs.llm_candidate_analysis.outputFiles['candidate_analysis.json'] }}"
      dependencies.json: "{{ outputs.detect_dependencies.outputFiles['dependencies.json'] }}"
    outputFiles:
      - candidates_result.json
    script: |
      import json
      import os
      import psycopg2
      
      db_url = os.environ.get('DATABASE_URL')
      watcher_id = os.environ.get('WATCHER_ID', '')
      
      with open('discovery.json', 'r') as f:
          discovery = json.load(f)
      entities = discovery.get('entities', {})
      
      with open('candidate_analysis.json', 'r') as f:
          llm_analysis = json.load(f)
      llm_by_sig = {a['signature']: a for a in llm_analysis}
      
      with open('dependencies.json', 'r') as f:
          dependencies = json.load(f)
      
      type_mapping = {
          'http_endpoints': 'http_endpoint',
          'grpc_services': 'grpc_service',
          'graphql_resolvers': 'graphql_resolver',
          'cron_jobs': 'cron_job',
          'queue_workers': 'queue_worker',
          'serverless_functions': 'serverless_function',
          'websockets': 'websocket'
      }
      
      def build_signature(etype, entity):
          if etype == 'http_endpoints':
              return f"{entity.get('method', 'GET')}:{entity.get('path', '/')}"
          elif etype == 'cron_jobs':
              return f"cron:{entity.get('schedule', 'unknown')}"
          elif etype == 'queue_workers':
              return f"queue:{entity.get('queue_name', 'default')}"
          elif etype == 'serverless_functions':
              return f"fn:{entity.get('function_name', 'handler')}"
          elif etype == 'grpc_services':
              return f"grpc:{entity.get('service_name', 'unknown')}"
          elif etype == 'graphql_resolvers':
              return f"gql:{entity.get('operation_type', 'query')}"
          elif etype == 'websockets':
              return f"ws:{entity.get('event_name', 'connection')}"
          return f"unknown:{entity.get('source_file', '')}"
      
      conn = psycopg2.connect(db_url, sslmode='require')
      cur = conn.cursor()
      
      seen = set()
      count = 0
      
      for etype, elist in entities.items():
          db_type = type_mapping.get(etype, etype.rstrip('s'))
          
          for e in elist:
              sig = build_signature(etype, e)
              
              key = (db_type, sig)
              if key in seen:
                  continue
              seen.add(key)
              
              llm_data = llm_by_sig.get(sig, {})
              deps_data = dependencies.get(sig, {})
              
              if etype == 'http_endpoints':
                  name = f"{e.get('method', 'GET')} {e.get('path', '/')}"
                  method = e.get('method')
                  route = e.get('path')
              else:
                  name = sig
                  method = None
                  route = None
              
              caller_count = deps_data.get('caller_count', 0)
              dependency_count = deps_data.get('dependency_count', 0)
              depends_on_sigs = deps_data.get('depends_on_signatures', [])
              called_by_sigs = deps_data.get('called_by_signatures', [])
              
              llm_risk = llm_data.get('llm_risk_score', 0.5) or 0.5
              
              # Calculate zombie_score (0-100):
              # Base: LLM risk score (0.0-1.0 â†’ 0-100)
              # The LLM already considers caller_count context in its analysis
              # Actual zombie confirmation comes from W2 observation (real traffic data)
              # 
              # Small bonus for internal callers (proves the code is actively used)
              # but NO penalty for zero callers (APIs are called externally, 
              # and even internal code might be called via reflection/dynamic dispatch)
              base_score = int(llm_risk * 100)
              
              if caller_count > 0:
                  # Has internal callers: slightly reduce risk (evidence of use)
                  caller_bonus = min(15, caller_count * 3)  # max 15 point reduction
                  zombie_score = max(0, base_score - caller_bonus)
              else:
                  # No internal callers: just use LLM score (no artificial boost)
                  zombie_score = base_score
              
              cur.execute("""
                  INSERT INTO zombie_candidates (
                      watcher_id, entity_type, entity_signature, entity_name,
                      file_path, method, route_path, schedule, queue_name, framework,
                      llm_purpose, llm_risk_score, llm_risk_reasoning,
                      depends_on_signatures, dependency_count, called_by_signatures, caller_count, zombie_score
                  ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                  ON CONFLICT (watcher_id, entity_type, entity_signature) DO UPDATE SET
                      entity_name = EXCLUDED.entity_name,
                      llm_purpose = EXCLUDED.llm_purpose,
                      llm_risk_score = EXCLUDED.llm_risk_score,
                      llm_risk_reasoning = EXCLUDED.llm_risk_reasoning,
                      depends_on_signatures = EXCLUDED.depends_on_signatures,
                      dependency_count = EXCLUDED.dependency_count,
                      called_by_signatures = EXCLUDED.called_by_signatures,
                      caller_count = EXCLUDED.caller_count,
                      zombie_score = EXCLUDED.zombie_score,
                      scan_count = zombie_candidates.scan_count + 1,
                      updated_at = NOW()
              """, (
                  watcher_id,
                  db_type,
                  sig,
                  name,
                  e.get('source_file', ''),
                  method,
                  route,
                  e.get('schedule') if etype == 'cron_jobs' else None,
                  e.get('queue_name') if etype == 'queue_workers' else None,
                  e.get('framework'),
                  llm_data.get('llm_purpose'),
                  llm_data.get('llm_risk_score'),
                  llm_data.get('llm_risk_reasoning'),
                  depends_on_sigs if depends_on_sigs else None,
                  dependency_count,
                  called_by_sigs if called_by_sigs else None,
                  caller_count,
                  zombie_score
              ))
              count += 1
      
      conn.commit()
      cur.close()
      conn.close()
      
      result = {"candidates_stored": count, "watcher_id": watcher_id}
      with open('candidates_result.json', 'w') as f:
          json.dump(result, f, indent=2)
      
      print(f"Candidates stored: {count}")

  # ============================================================================
  # OUTPUT
  # ============================================================================
  - id: output_success
    type: io.kestra.plugin.core.output.OutputValues
    description: Return creation results
    values:
      watcher_id: "{{ inputs.watcher_id }}"
      watcher_name: "{{ inputs.watcher_name }}"
      repo_url: "{{ inputs.repo_url }}"
      status: "created"
      execution_id: "{{ execution.id }}"

errors:
  - id: creation_failed
    type: io.kestra.plugin.core.output.OutputValues
    values:
      watcher_id: "{{ inputs.watcher_id }}"
      status: "creation_failed"
      error: "Watcher creation workflow failed"
      execution_id: "{{ execution.id }}"

triggers: []

pluginDefaults:
  - type: io.kestra.plugin.scripts.python.Script
    values:
      docker:
        pullPolicy: IF_NOT_PRESENT
  - type: io.kestra.plugin.git.Clone
    values:
      depth: 1
