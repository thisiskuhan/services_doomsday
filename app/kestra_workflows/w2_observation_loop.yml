id: w2_observation_loop
namespace: doomsday.guardians

description: |
  W2 OBSERVATION LOOP - Continuous Traffic Monitoring for Zombie Candidates
  
  Scheduled workflow that continuously monitors zombie candidates for traffic
  evidence. Queries configured observability sources (Prometheus, Loki, Grafana,
  Datadog), updating zombie scores based on consecutive zero-traffic observations.
  
  WORKFLOW STAGES:
  1. POLL DUE CANDIDATES - Query DB for candidates where next_poll_at <= NOW
  2. OBSERVE EACH - ForEach loop with parallel execution (limit: 10)
     - PROMETHEUS: Query metrics for request_count > 0
     - LOKI: Query logs for HTTP traffic entries
     - GRAFANA: Query Grafana Cloud Prometheus API
     - DATADOG: Query Datadog Metrics API
  3. STORE EVIDENCE - Insert observation_event with traffic status
  4. UPDATE SCORE - Increment zombie_score on consecutive zero-hits
  
  ZOMBIE SCORE LOGIC:
  - Traffic detected: Reset consecutive_zero_traffic to 0
  - No traffic: Increment consecutive_zero_traffic
  - Score formula: MIN(zombie_score + 5, 100) per zero observation
  - Analysis trigger: When observation_end_at reached (W3 picks up)
  
  TIME RANGE:
  - Each source is queried with the candidate's scan_frequency_minutes as the
    lookback window. E.g., if scan_freq=60, query last 60 minutes of data.
  
  SUPPORTED OBSERVABILITY SOURCES (4 only):
  - prometheus: PromQL via /api/v1/query
  - loki: LogQL via /loki/api/v1/query_range
  - grafana: Grafana Cloud Prometheus API
  - datadog: Datadog Metrics API /api/v1/query

labels:
  team: doomsday
  category: watcher-management
  stage: observation
  version: "2.0"

variables:
  max_candidates_per_run: 100
  concurrency_limit: 10
  suspect_threshold: 3

tasks:
  # ============================================================================
  # STAGE 1: Poll for Due Candidates
  # ============================================================================
  - id: poll_due_candidates
    type: io.kestra.plugin.scripts.python.Script
    description: Find all zombie_candidates due for observation
    retry:
      type: exponential
      maxAttempts: 3
      interval: PT5S
      maxInterval: PT30S
    timeout: PT2M
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
    containerImage: python:3.11-slim
    beforeCommands:
      - pip install -q psycopg2-binary
    env:
      DATABASE_URL: "{{ secret('DATABASE_URL') }}"
      MAX_CANDIDATES: "{{ vars.max_candidates_per_run }}"
    outputFiles:
      - due_candidates.json
      - poll_stats.json
    script: |
      import os
      import json
      import psycopg2
      from datetime import datetime, timezone
      
      db_url = os.environ.get('DATABASE_URL')
      max_candidates = int(os.environ.get('MAX_CANDIDATES', '100'))
      
      conn = psycopg2.connect(db_url, sslmode='require')
      cur = conn.cursor()
      
      now = datetime.now(timezone.utc)
      
      # Get count of active candidates
      cur.execute("""
          SELECT COUNT(*) FROM zombie_candidates WHERE status = 'active'
      """)
      total_active = cur.fetchone()[0]
      
      # Find due candidates
      cur.execute("""
          SELECT 
              zc.candidate_id,
              zc.watcher_id,
              zc.entity_type,
              zc.entity_signature,
              zc.entity_name,
              zc.file_path,
              zc.method,
              zc.route_path,
              zc.scan_frequency_minutes,
              zc.observation_count,
              zc.consecutive_zero_traffic,
              zc.health_check_count,
              zc.zombie_score,
              zc.has_traffic,
              w.observability_urls,
              w.watcher_name,
              w.repo_name,
              w.application_url
          FROM zombie_candidates zc
          JOIN watchers w ON zc.watcher_id = w.watcher_id
          WHERE zc.status = 'active'
            AND zc.next_observation_at <= NOW()
            AND (zc.observation_end_at IS NULL OR zc.observation_end_at > NOW())
          ORDER BY zc.next_observation_at ASC
          LIMIT %s
      """, (max_candidates,))
      
      columns = [desc[0] for desc in cur.description]
      rows = cur.fetchall()
      
      candidates = []
      for row in rows:
          candidate = dict(zip(columns, row))
          # Parse observability_urls
          obs_urls = candidate.get('observability_urls')
          if isinstance(obs_urls, str):
              try:
                  candidate['observability_urls'] = json.loads(obs_urls)
              except:
                  candidate['observability_urls'] = []
          candidates.append(candidate)
      
      cur.close()
      conn.close()
      
      # Write outputs
      with open('due_candidates.json', 'w') as f:
          json.dump(candidates, f, indent=2, default=str)
      
      stats = {
          "poll_timestamp": now.isoformat(),
          "due_candidates_found": len(candidates),
          "total_active_candidates": total_active,
          "max_candidates_limit": max_candidates
      }
      with open('poll_stats.json', 'w') as f:
          json.dump(stats, f, indent=2)
      
      print(f"[POLL] Found {len(candidates)}/{total_active} active candidates due for observation")

  # ============================================================================
  # STAGE 2: Check if Any Candidates Found
  # ============================================================================
  - id: check_has_candidates
    type: io.kestra.plugin.core.flow.If
    description: Skip observation if no candidates are due
    condition: "{{ read(outputs.poll_due_candidates.outputFiles['due_candidates.json']) | json | length > 0 }}"
    then:
      # ========================================================================
      # STAGE 3: Process Each Candidate
      # ========================================================================
      - id: process_candidates
        type: io.kestra.plugin.core.flow.ForEach
        description: Process each due candidate in parallel
        concurrencyLimit: 10
        values: "{{ read(outputs.poll_due_candidates.outputFiles['due_candidates.json']) }}"
        tasks:
          # Single comprehensive task - no WorkingDirectory needed
          - id: observe_and_store
            type: io.kestra.plugin.scripts.python.Script
            description: Query sources, store evidence, update candidate
            retry:
              type: exponential
              maxAttempts: 2
              interval: PT5S
              maxInterval: PT30S
            timeout: PT5M
            taskRunner:
              type: io.kestra.plugin.scripts.runner.docker.Docker
            containerImage: python:3.11-slim
            beforeCommands:
              - pip install -q psycopg2-binary requests
            env:
              CANDIDATE_DATA: "{{ taskrun.value }}"
              DATABASE_URL: "{{ secret('DATABASE_URL') }}"
              SUSPECT_THRESHOLD: "{{ vars.suspect_threshold }}"
              QUERY_TIMEOUT: "30"
            script: |
              import os
              import json
              import uuid
              import time
              import requests
              import psycopg2
              from datetime import datetime, timezone, timedelta
              from concurrent.futures import ThreadPoolExecutor, as_completed
              
              # =================================================================
              # CONFIGURATION
              # =================================================================
              db_url = os.environ.get('DATABASE_URL')
              suspect_threshold = int(os.environ.get('SUSPECT_THRESHOLD', '3'))
              query_timeout = int(os.environ.get('QUERY_TIMEOUT', '30'))
              
              candidate_str = os.environ.get('CANDIDATE_DATA', '{}')
              candidate = json.loads(candidate_str)
              
              # Generate batch ID
              batch_id = str(uuid.uuid4())
              now = datetime.now(timezone.utc)
              
              # Extract candidate info
              candidate_id = candidate.get('candidate_id')
              watcher_id = candidate.get('watcher_id')
              entity_type = candidate.get('entity_type')
              entity_sig = candidate.get('entity_signature', 'unknown')
              method = candidate.get('method', 'GET')
              route_path = candidate.get('route_path', '/')
              scan_freq = candidate.get('scan_frequency_minutes', 60)
              prev_obs_count = candidate.get('observation_count', 0)
              prev_consecutive_zero = candidate.get('consecutive_zero_traffic', 0)
              prev_zombie_score = candidate.get('zombie_score', 0)
              app_url = candidate.get('application_url')
              
              # Parse observability sources
              obs_urls = candidate.get('observability_urls') or []
              if isinstance(obs_urls, str):
                  try:
                      obs_urls = json.loads(obs_urls)
                  except:
                      obs_urls = []
              
              # Helper to parse Java Map toString format: {key=value, key2=value2}
              def parse_java_map(s):
                  if not isinstance(s, str):
                      return s
                  s = s.strip()
                  if not (s.startswith('{') and s.endswith('}')):
                      return s
                  # Check if it's JSON (has "key": pattern) vs Java Map (has key= pattern)
                  # Java Map: {url=https://..., type=prometheus}
                  # JSON: {"url": "https://...", "type": "prometheus"}
                  import re
                  is_json = bool(re.search(r'"\w+":', s))
                  is_java_map = bool(re.search(r'^\{[a-zA-Z_]\w*=', s))
                  if is_java_map and not is_json:
                      # Parse Java Map format
                      inner = s[1:-1]
                      result = {}
                      # Use regex to split on ", " only when followed by key=
                      parts = re.split(r', (?=[a-zA-Z_]\w*=)', inner)
                      for part in parts:
                          if '=' in part:
                              k, v = part.split('=', 1)
                              result[k.strip()] = v.strip()
                      return result
                  return s
              
              # =================================================================
              # BUILD SOURCE CONFIGURATIONS
              # =================================================================
              sources = []
              for idx, item in enumerate(obs_urls):
                  # Handle Java Map string format
                  url = parse_java_map(item) if isinstance(item, str) else item
                  
                  if isinstance(url, dict):
                      sources.append({
                          "source_type": url.get("type", "prometheus"),
                          "source_name": url.get("name", f"source_{idx}"),
                          "source_url": url.get("url", ""),
                          "source_token": url.get("token", ""),
                          "source_user_id": url.get("userId", ""),  # For Grafana Cloud Basic Auth
                          "query_template": url.get("query", "")
                      })
                  elif isinstance(url, str) and url.strip():
                      # Plain URL string
                      if url.startswith('http'):
                          sources.append({
                              "source_type": "prometheus",
                              "source_name": f"prometheus_{idx}",
                              "source_url": url
                          })
                      # else: skip invalid format
              
              if not sources:
                  sources.append({
                      "source_type": "no_data",
                      "source_name": "no_sources",
                      "source_url": ""
                  })
              
              print(f"[PREP] Batch {batch_id[:8]} | Candidate {candidate_id} | {len(sources)} sources")
              
              # =================================================================
              # QUERY HANDLERS (Prometheus, Loki, Grafana, Datadog)
              # =================================================================
              
              # Time range for queries
              # Use scan_frequency_minutes as base, but extend to 7 days for range queries
              # This catches traffic even if metrics push is delayed/sporadic
              time_range_minutes = scan_freq if scan_freq and scan_freq > 0 else 60
              end_time = now
              # For range queries, use 7 days lookback to catch any traffic
              start_time = now - timedelta(days=7)
              
              def query_prometheus(source):
                  """
                  Query Prometheus for HTTP request metrics.
                  API: GET /api/v1/query with PromQL
                  Auth: 
                    - Grafana Cloud: Basic Auth with userId:token (glc_* tokens)
                    - Standard: Bearer token
                  """
                  import base64
                  result = {
                      "source_type": "prometheus",
                      "source_name": source.get('source_name', 'prometheus'),
                      "source_url": source.get('source_url', ''),
                      "traffic_detected": False,
                      "raw_request_count": 0,
                      "error_type": None,
                      "error_message": None
                  }
                  url = source.get('source_url', '').rstrip('/')
                  if not url:
                      result['error_type'] = 'no_url'
                      result['error_message'] = 'No Prometheus URL configured'
                      return result
                  
                  if not url.startswith('http'):
                      result['error_type'] = 'invalid_url'
                      result['error_message'] = 'Invalid URL format (must start with http/https)'
                      return result
                  
                  # Build PromQL - query raw metric values (not increase) for range query
                  # Try multiple common label names: route, path, uri, endpoint
                  if route_path:
                      # Strip dynamic path parameters like [country], [id], :id, etc.
                      # These are Next.js/Express style dynamic segments
                      import re
                      clean_route = re.sub(r'/\[[^\]]+\]', '', route_path)  # Remove /[param]
                      clean_route = re.sub(r'/:[^/]+', '', clean_route)     # Remove /:param
                      clean_route = clean_route.rstrip('/')                  # Remove trailing slash
                      if not clean_route:
                          clean_route = route_path.split('/')[1] if '/' in route_path else route_path
                      # Query http_requests_total with route filter
                      promql = 'http_requests_total{route=~".*' + clean_route + '.*"}'
                  else:
                      # Query all http_requests_total
                      promql = 'http_requests_total'
                  result['query_expression'] = promql
                  
                  # Build auth headers
                  headers = {}
                  token = source.get('source_token', '')
                  user_id = source.get('source_user_id', '')
                  
                  if token:
                      # Grafana Cloud tokens start with glc_ - use Basic Auth
                      if token.startswith('glc_') or token.startswith('glsa_'):
                          if user_id:
                              basic_auth = base64.b64encode(f"{user_id}:{token}".encode()).decode()
                              headers['Authorization'] = f'Basic {basic_auth}'
                          else:
                              # Try to extract userId from token if not provided
                              try:
                                  import json as json_mod
                                  token_data = json_mod.loads(base64.b64decode(token[4:]).decode())
                                  extracted_id = token_data.get('o', '')
                                  if extracted_id:
                                      basic_auth = base64.b64encode(f"{extracted_id}:{token}".encode()).decode()
                                      headers['Authorization'] = f'Basic {basic_auth}'
                                  else:
                                      headers['Authorization'] = f'Bearer {token}'
                              except:
                                  headers['Authorization'] = f'Bearer {token}'
                      else:
                          headers['Authorization'] = f'Bearer {token}'
                  
                  try:
                      # Use range query instead of instant query to handle stale data
                      # This ensures we catch traffic even if metrics aren't being actively scraped
                      from_ts = int(start_time.timestamp())
                      to_ts = int(end_time.timestamp())
                      
                      start = time.time()
                      resp = requests.get(
                          f"{url}/api/v1/query_range", 
                          params={
                              "query": promql,
                              "start": str(from_ts),
                              "end": str(to_ts),
                              "step": "60"  # 1 minute resolution
                          }, 
                          headers=headers if headers else None,
                          timeout=query_timeout
                      )
                      result['query_response_time_ms'] = int((time.time() - start) * 1000)
                      
                      if resp.status_code == 200:
                          data = resp.json()
                          if data.get('status') == 'success':
                              # For range queries, sum all values across all results
                              for r in data.get('data', {}).get('result', []):
                                  for val in r.get('values', []):
                                      if len(val) > 1:
                                          try:
                                              result['raw_request_count'] += max(0, int(float(val[1])))
                                          except:
                                              pass
                              result['traffic_detected'] = result['raw_request_count'] > 0
                      elif resp.status_code == 401:
                          result['error_type'] = 'auth_error'
                          result['error_message'] = 'Authentication failed - check API token'
                      elif resp.status_code == 403:
                          result['error_type'] = 'forbidden'
                          result['error_message'] = 'Access denied - insufficient permissions'
                      elif resp.status_code == 404:
                          result['error_type'] = 'not_found'
                          result['error_message'] = 'Prometheus API endpoint not found'
                      else:
                          result['error_type'] = 'http_error'
                          result['error_message'] = f"HTTP {resp.status_code}: {resp.text[:100]}"
                  except requests.Timeout:
                      result['error_type'] = 'timeout'
                      result['error_message'] = f"Timeout after {query_timeout}s"
                  except requests.exceptions.ConnectionError as e:
                      result['error_type'] = 'connection_error'
                      result['error_message'] = f'Connection failed: {str(e)[:100]}'
                  except requests.exceptions.JSONDecodeError:
                      result['error_type'] = 'invalid_response'
                      result['error_message'] = 'Invalid JSON response from Prometheus'
                  except Exception as e:
                      result['error_type'] = 'error'
                      result['error_message'] = str(e)[:200]
                  return result
              
              def query_loki(source):
                  """
                  Query Loki for log-based traffic evidence.
                  API: GET /loki/api/v1/query_range
                  Params: query (LogQL), start, end (nanoseconds)
                  Auth: 
                    - Grafana Cloud: Basic Auth with userId:token (glc_* tokens)
                    - Standard: Bearer token or X-Scope-OrgID for multi-tenant
                  """
                  import base64
                  result = {
                      "source_type": "loki",
                      "source_name": source.get('source_name', 'loki'),
                      "source_url": source.get('source_url', ''),
                      "traffic_detected": False,
                      "raw_request_count": 0,
                      "error_type": None,
                      "error_message": None
                  }
                  url = source.get('source_url', '').rstrip('/')
                  if not url:
                      result['error_type'] = 'no_url'
                      result['error_message'] = 'No Loki URL configured'
                      return result
                  
                  if not url.startswith('http'):
                      result['error_type'] = 'invalid_url'
                      result['error_message'] = 'Invalid URL format (must start with http/https)'
                      return result
                  
                  # Build LogQL - count log entries matching the route
                  time_range = str(time_range_minutes) + 'm'
                  if route_path:
                      # Count logs containing the route path
                      logql = 'sum(count_over_time({job=~".+"} |~ "' + route_path + '" [' + time_range + ']))'
                  else:
                      # Count all HTTP-related logs
                      logql = 'sum(count_over_time({job=~".+"} |~ "HTTP|GET|POST|PUT|DELETE" [' + time_range + ']))'
                  result['query_expression'] = logql
                  
                  # Build auth headers
                  headers = {}
                  token = source.get('source_token', '')
                  user_id = source.get('source_user_id', '')
                  
                  if token:
                      # Grafana Cloud tokens start with glc_ - use Basic Auth
                      if token.startswith('glc_') or token.startswith('glsa_'):
                          if user_id:
                              basic_auth = base64.b64encode(f"{user_id}:{token}".encode()).decode()
                              headers['Authorization'] = f'Basic {basic_auth}'
                          else:
                              # Try to extract userId from token if not provided
                              try:
                                  import json as json_mod
                                  token_data = json_mod.loads(base64.b64decode(token[4:]).decode())
                                  extracted_id = token_data.get('o', '')
                                  if extracted_id:
                                      basic_auth = base64.b64encode(f"{extracted_id}:{token}".encode()).decode()
                                      headers['Authorization'] = f'Basic {basic_auth}'
                                  else:
                                      headers['Authorization'] = f'Bearer {token}'
                              except:
                                  headers['Authorization'] = f'Bearer {token}'
                      else:
                          # Could be org ID for multi-tenant Loki
                          headers['X-Scope-OrgID'] = token
                  
                  try:
                      start_ts = int(start_time.timestamp() * 1e9)  # Loki uses nanoseconds
                      end_ts = int(end_time.timestamp() * 1e9)
                      
                      start = time.time()
                      resp = requests.get(
                          f"{url}/loki/api/v1/query_range",
                          params={
                              "query": logql,
                              "start": str(start_ts),
                              "end": str(end_ts),
                              "step": f"{max(60, time_range_minutes * 60 // 10)}s"  # Reasonable step
                          },
                          headers=headers if headers else None,
                          timeout=query_timeout
                      )
                      result['query_response_time_ms'] = int((time.time() - start) * 1000)
                      
                      if resp.status_code == 200:
                          data = resp.json()
                          if data.get('status') == 'success':
                              result_data = data.get('data', {})
                              result_type = result_data.get('resultType', '')
                              
                              if result_type == 'matrix':
                                  # Matrix result from count_over_time
                                  for series in result_data.get('result', []):
                                      for val in series.get('values', []):
                                          try:
                                              result['raw_request_count'] += int(float(val[1]))
                                          except:
                                              pass
                              elif result_type == 'streams':
                                  # Stream result - count log lines
                                  for stream in result_data.get('result', []):
                                      result['raw_request_count'] += len(stream.get('values', []))
                              elif result_type == 'vector':
                                  # Instant vector
                                  for r in result_data.get('result', []):
                                      val = r.get('value', [None, '0'])
                                      if len(val) > 1:
                                          try:
                                              result['raw_request_count'] += int(float(val[1]))
                                          except:
                                              pass
                              
                              result['traffic_detected'] = result['raw_request_count'] > 0
                      elif resp.status_code == 401:
                          result['error_type'] = 'auth_error'
                          result['error_message'] = 'Authentication failed - check Loki token'
                      elif resp.status_code == 403:
                          result['error_type'] = 'forbidden'
                          result['error_message'] = 'Access denied to Loki'
                      elif resp.status_code == 404:
                          result['error_type'] = 'not_found'
                          result['error_message'] = 'Loki API endpoint not found - check URL'
                      elif resp.status_code == 400:
                          result['error_type'] = 'bad_request'
                          result['error_message'] = f'Invalid LogQL query: {resp.text[:100]}'
                      else:
                          result['error_type'] = 'http_error'
                          result['error_message'] = f"HTTP {resp.status_code}: {resp.text[:100]}"
                  except requests.Timeout:
                      result['error_type'] = 'timeout'
                      result['error_message'] = f"Loki timeout after {query_timeout}s"
                  except requests.exceptions.ConnectionError as e:
                      result['error_type'] = 'connection_error'
                      result['error_message'] = f'Loki connection failed: {str(e)[:100]}'
                  except requests.exceptions.JSONDecodeError:
                      result['error_type'] = 'invalid_response'
                      result['error_message'] = 'Invalid JSON response from Loki'
                  except Exception as e:
                      result['error_type'] = 'error'
                      result['error_message'] = f'Loki error: {str(e)[:200]}'
                  return result
              
              def query_grafana(source):
                  """
                  Query Grafana Cloud via its unified API.
                  Grafana proxies to Prometheus/Loki datasources.
                  API: GET /api/prom/api/v1/query
                  Auth: 
                    - Grafana Cloud: Basic Auth with userId:token (glc_* tokens)
                    - Standard: Bearer token
                  """
                  import base64
                  result = {
                      "source_type": "grafana",
                      "source_name": source.get('source_name', 'grafana'),
                      "source_url": source.get('source_url', ''),
                      "traffic_detected": False,
                      "raw_request_count": 0,
                      "error_type": None,
                      "error_message": None
                  }
                  url = source.get('source_url', '').rstrip('/')
                  if not url:
                      result['error_type'] = 'no_url'
                      result['error_message'] = 'No Grafana URL configured'
                      return result
                  
                  if not url.startswith('http'):
                      result['error_type'] = 'invalid_url'
                      result['error_message'] = 'Invalid URL format (must start with http/https)'
                      return result
                  
                  token = source.get('source_token', '')
                  if not token:
                      result['error_type'] = 'no_token'
                      result['error_message'] = 'Grafana requires API token (glc_*)'
                      return result
                  
                  # Build auth headers
                  headers = {'Content-Type': 'application/json'}
                  user_id = source.get('source_user_id', '')
                  
                  # Grafana Cloud tokens start with glc_ - use Basic Auth
                  if token.startswith('glc_') or token.startswith('glsa_'):
                      if user_id:
                          basic_auth = base64.b64encode(f"{user_id}:{token}".encode()).decode()
                          headers['Authorization'] = f'Basic {basic_auth}'
                      else:
                          # Try to extract userId from token if not provided
                          try:
                              import json as json_mod
                              token_data = json_mod.loads(base64.b64decode(token[4:]).decode())
                              extracted_id = token_data.get('o', '')
                              if extracted_id:
                                  basic_auth = base64.b64encode(f"{extracted_id}:{token}".encode()).decode()
                                  headers['Authorization'] = f'Basic {basic_auth}'
                              else:
                                  headers['Authorization'] = f'Bearer {token}'
                          except:
                              headers['Authorization'] = f'Bearer {token}'
                  else:
                      headers['Authorization'] = f'Bearer {token}'
                  
                  # Build PromQL query for Grafana's Prometheus datasource
                  # Query raw metric values for range query
                  if route_path:
                      # Strip dynamic path parameters like [country], [id], :id, etc.
                      import re
                      clean_route = re.sub(r'/\[[^\]]+\]', '', route_path)  # Remove /[param]
                      clean_route = re.sub(r'/:[^/]+', '', clean_route)     # Remove /:param
                      clean_route = clean_route.rstrip('/')                  # Remove trailing slash
                      if not clean_route:
                          clean_route = route_path.split('/')[1] if '/' in route_path else route_path
                      promql = 'http_requests_total{route=~".*' + clean_route + '.*"}'
                  else:
                      promql = 'http_requests_total'
                  result['query_expression'] = promql
                  
                  try:
                      # Try Grafana Cloud Prometheus API endpoint
                      # Format: https://<stack>.grafana.net/api/prom/api/v1/query_range
                      prom_url = url
                      if 'grafana.net' in url or 'grafana.com' in url:
                          # Grafana Cloud - use the Prometheus API path
                          if '/api/prom' not in url:
                              prom_url = f"{url}/api/prom"
                      
                      # Use range query to handle stale/historical data
                      from_ts = int(start_time.timestamp())
                      to_ts = int(end_time.timestamp())
                      
                      start = time.time()
                      resp = requests.get(
                          f"{prom_url}/api/v1/query_range",
                          params={
                              "query": promql,
                              "start": str(from_ts),
                              "end": str(to_ts),
                              "step": "60"
                          },
                          headers=headers,
                          timeout=query_timeout
                      )
                      result['query_response_time_ms'] = int((time.time() - start) * 1000)
                      
                      if resp.status_code == 200:
                          data = resp.json()
                          if data.get('status') == 'success':
                              # For range queries, sum all values across all results
                              for r in data.get('data', {}).get('result', []):
                                  for val in r.get('values', []):
                                      if len(val) > 1:
                                          try:
                                              result['raw_request_count'] += max(0, int(float(val[1])))
                                          except:
                                              pass
                              result['traffic_detected'] = result['raw_request_count'] > 0
                      elif resp.status_code == 401:
                          result['error_type'] = 'auth_error'
                          result['error_message'] = 'Grafana auth failed - check API token'
                      elif resp.status_code == 403:
                          result['error_type'] = 'forbidden'
                          result['error_message'] = 'Grafana access denied - check permissions'
                      elif resp.status_code == 404:
                          result['error_type'] = 'not_found'
                          result['error_message'] = 'Grafana API endpoint not found'
                      else:
                          result['error_type'] = 'http_error'
                          result['error_message'] = f"HTTP {resp.status_code}: {resp.text[:100]}"
                  except requests.Timeout:
                      result['error_type'] = 'timeout'
                      result['error_message'] = f"Grafana timeout after {query_timeout}s"
                  except requests.exceptions.ConnectionError as e:
                      result['error_type'] = 'connection_error'
                      result['error_message'] = f'Grafana connection failed: {str(e)[:100]}'
                  except requests.exceptions.JSONDecodeError:
                      result['error_type'] = 'invalid_response'
                      result['error_message'] = 'Invalid JSON response from Grafana'
                  except Exception as e:
                      result['error_type'] = 'error'
                      result['error_message'] = f'Grafana error: {str(e)[:200]}'
                  return result
              
              def query_datadog(source):
                  """
                  Query Datadog Metrics API for traffic data.
                  API: GET /api/v1/query
                  Params: from, to (unix seconds), query (DD query string)
                  Auth: DD-API-KEY and DD-APPLICATION-KEY headers
                  Site: api.datadoghq.com (US), api.datadoghq.eu (EU), etc.
                  """
                  result = {
                      "source_type": "datadog",
                      "source_name": source.get('source_name', 'datadog'),
                      "source_url": source.get('source_url', ''),
                      "traffic_detected": False,
                      "raw_request_count": 0,
                      "error_type": None,
                      "error_message": None
                  }
                  url = source.get('source_url', '').rstrip('/')
                  if not url:
                      # Default to US Datadog
                      url = 'https://api.datadoghq.com'
                  
                  if not url.startswith('http'):
                      result['error_type'] = 'invalid_url'
                      result['error_message'] = 'Invalid URL format (must start with http/https)'
                      return result
                  
                  token = source.get('source_token', '')
                  if not token:
                      result['error_type'] = 'no_token'
                      result['error_message'] = 'Datadog requires API key'
                      return result
                  
                  # Token format: "api_key:app_key" or just "api_key"
                  if ':' in token:
                      api_key, app_key = token.split(':', 1)
                  else:
                      api_key = token
                      app_key = ''
                  
                  headers = {
                      'DD-API-KEY': api_key,
                      'Content-Type': 'application/json'
                  }
                  if app_key:
                      headers['DD-APPLICATION-KEY'] = app_key
                  
                  # Build Datadog metric query
                  # Common metrics: trace.http.request.hits, http.requests, etc.
                  # Note: Using string concatenation to avoid Kestra Pebble template conflict with curly braces
                  if route_path:
                      dd_query = 'sum:trace.http.request.hits{resource_name:*' + route_path + '*}.as_count()'
                  else:
                      dd_query = 'sum:trace.http.request.hits{*}.as_count()'
                  result['query_expression'] = dd_query
                  
                  try:
                      from_ts = int(start_time.timestamp())
                      to_ts = int(end_time.timestamp())
                      
                      start = time.time()
                      resp = requests.get(
                          f"{url}/api/v1/query",
                          params={
                              "from": str(from_ts),
                              "to": str(to_ts),
                              "query": dd_query
                          },
                          headers=headers,
                          timeout=query_timeout
                      )
                      result['query_response_time_ms'] = int((time.time() - start) * 1000)
                      
                      if resp.status_code == 200:
                          data = resp.json()
                          if data.get('status') == 'ok':
                              # Sum all points from all series
                              for series in data.get('series', []):
                                  for point in series.get('pointlist', []):
                                      if len(point) > 1 and point[1] is not None:
                                          try:
                                              result['raw_request_count'] += int(float(point[1]))
                                          except:
                                              pass
                              result['traffic_detected'] = result['raw_request_count'] > 0
                          elif 'error' in data:
                              result['error_type'] = 'api_error'
                              result['error_message'] = f"Datadog: {data.get('error', 'Unknown error')[:100]}"
                      elif resp.status_code == 401:
                          result['error_type'] = 'auth_error'
                          result['error_message'] = 'Datadog auth failed - check API key'
                      elif resp.status_code == 403:
                          result['error_type'] = 'forbidden'
                          result['error_message'] = 'Datadog access denied - check permissions'
                      elif resp.status_code == 400:
                          result['error_type'] = 'bad_request'
                          result['error_message'] = f'Invalid Datadog query: {resp.text[:100]}'
                      else:
                          result['error_type'] = 'http_error'
                          result['error_message'] = f"HTTP {resp.status_code}: {resp.text[:100]}"
                  except requests.Timeout:
                      result['error_type'] = 'timeout'
                      result['error_message'] = f"Datadog timeout after {query_timeout}s"
                  except requests.exceptions.ConnectionError as e:
                      result['error_type'] = 'connection_error'
                      result['error_message'] = f'Datadog connection failed: {str(e)[:100]}'
                  except requests.exceptions.JSONDecodeError:
                      result['error_type'] = 'invalid_response'
                      result['error_message'] = 'Invalid JSON response from Datadog'
                  except Exception as e:
                      result['error_type'] = 'error'
                      result['error_message'] = f'Datadog error: {str(e)[:200]}'
                  return result
              
              def query_no_data(source):
                  return {
                      "source_type": "no_data",
                      "source_name": source.get('source_name', 'no_data'),
                      "traffic_detected": False,
                      "error_type": "no_sources",
                      "error_message": "No observability sources configured"
                  }
              
              # Only 4 observability sources supported
              HANDLERS = {
                  "prometheus": query_prometheus,
                  "loki": query_loki,
                  "grafana": query_grafana,
                  "datadog": query_datadog,
                  "no_data": query_no_data
              }
              
              # =================================================================
              # QUERY ALL SOURCES IN PARALLEL
              # =================================================================
              results = []
              
              def query_source(src):
                  handler = HANDLERS.get(src.get('source_type', 'no_data'), query_no_data)
                  r = handler(src)
                  r['observation_batch_id'] = batch_id
                  r['candidate_id'] = candidate_id
                  r['watcher_id'] = watcher_id
                  r['observed_at'] = datetime.now(timezone.utc).isoformat()
                  return r
              
              with ThreadPoolExecutor(max_workers=min(10, len(sources))) as executor:
                  futures = {executor.submit(query_source, s): s for s in sources}
                  for future in as_completed(futures):
                      try:
                          results.append(future.result())
                      except Exception as e:
                          src = futures[future]
                          results.append({
                              "source_type": src.get('source_type', 'unknown'),
                              "traffic_detected": False,
                              "error_type": "executor_error",
                              "error_message": str(e)[:200]
                          })
              
              traffic_sources = sum(1 for r in results if r.get('traffic_detected'))
              print(f"[QUERY] {len(results)} sources | {traffic_sources} with traffic")
              
              # =================================================================
              # STORE TO DATABASE
              # =================================================================
              conn = psycopg2.connect(db_url, sslmode='require')
              cur = conn.cursor()
              
              # Prepare batch data for observation_events
              event_rows = []
              
              for sr in results:
                  event_rows.append((
                      candidate_id, watcher_id, batch_id,
                      sr.get('observed_at', now.isoformat()),
                      sr.get('source_type'), sr.get('source_name'), sr.get('source_url'),
                      sr.get('raw_request_count', 0), sr.get('traffic_detected', False),
                      sr.get('http_status'), sr.get('is_alive'), sr.get('health_check_latency_ms'),
                      sr.get('error_type'), sr.get('error_message'),
                      sr.get('query_expression'), sr.get('query_response_time_ms')
                  ))
              
              # Batch insert observation_events
              cur.executemany("""
                  INSERT INTO observation_events (
                      candidate_id, watcher_id, observation_batch_id, observed_at,
                      source_type, source_name, source_url,
                      raw_request_count, traffic_detected,
                      http_status, is_alive, health_check_latency_ms,
                      error_type, error_message, query_expression, query_response_time_ms
                  ) VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)
              """, event_rows)
              
              # Aggregate results
              total_sources = len(results)
              sources_with_traffic = sum(1 for r in results if r.get('traffic_detected'))
              sources_with_errors = sum(1 for r in results if r.get('error_type'))
              traffic_detected = sources_with_traffic > 0
              total_request_count = sum(r.get('raw_request_count', 0) for r in results)
              
              if sources_with_errors == total_sources:
                  observation_verdict = 'error'
              elif traffic_detected:
                  observation_verdict = 'traffic_found'
              else:
                  observation_verdict = 'no_traffic'
              
              # Store observation_summary
              cur.execute("""
                  INSERT INTO observation_summaries (
                      candidate_id, watcher_id, observation_batch_id, observed_at,
                      total_sources_queried, sources_with_traffic, sources_with_errors,
                      traffic_detected, total_request_count,
                      health_check_performed, health_check_alive, health_check_status,
                      observation_verdict
                  ) VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)
                  ON CONFLICT (candidate_id, observation_batch_id) DO NOTHING
              """, (
                  candidate_id, watcher_id, batch_id, now,
                  total_sources, sources_with_traffic, sources_with_errors,
                  traffic_detected, total_request_count,
                  False, None, None,  # health_check disabled
                  observation_verdict
              ))
              
              # =================================================================
              # UPDATE ZOMBIE_CANDIDATE
              # =================================================================
              new_obs_count = prev_obs_count + 1
              new_consecutive_zero = 0 if traffic_detected else prev_consecutive_zero + 1
              next_observation_at = now + timedelta(minutes=scan_freq)
              
              # Determine zombie verdict based on consecutive zero traffic observations
              consecutive_zero_threshold = 10  # Number of zero-traffic observations to mark as zombie
              if traffic_detected:
                  zombie_verdict = 'healthy'
                  new_zombie_score = max(0, prev_zombie_score - 10)
              elif new_consecutive_zero >= consecutive_zero_threshold:
                  zombie_verdict = 'zombie'
                  new_zombie_score = min(100, prev_zombie_score + 5)
              elif new_consecutive_zero >= suspect_threshold:
                  zombie_verdict = 'suspect'
                  new_zombie_score = min(100, prev_zombie_score + 2)
              else:
                  zombie_verdict = 'unknown'
                  new_zombie_score = prev_zombie_score
              
              log_entry = json.dumps({
                  "batch": batch_id[:8],
                  "at": now.isoformat(),
                  "traffic": traffic_detected,
                  "verdict": observation_verdict
              })
              
              cur.execute("""
                  UPDATE zombie_candidates SET
                      observation_count = %s,
                      consecutive_zero_traffic = %s,
                      health_check_count = health_check_count,
                      has_traffic = CASE WHEN %s THEN TRUE ELSE has_traffic END,
                      last_traffic_at = CASE WHEN %s THEN NOW() ELSE last_traffic_at END,
                      traffic_count = traffic_count + %s,
                      first_observed_at = COALESCE(first_observed_at, NOW()),
                      last_observed_at = NOW(),
                      next_observation_at = %s,
                      zombie_score = %s,
                      zombie_verdict = %s,
                      observation_log = (
                          CASE 
                              WHEN jsonb_array_length(COALESCE(observation_log, '[]'::jsonb)) >= 10 
                              THEN (observation_log #- '{0}') || %s::jsonb
                              ELSE COALESCE(observation_log, '[]'::jsonb) || %s::jsonb
                          END
                      ),
                      updated_at = NOW()
                  WHERE candidate_id = %s
              """, (
                  new_obs_count, new_consecutive_zero,
                  traffic_detected, traffic_detected, total_request_count,
                  next_observation_at, new_zombie_score, zombie_verdict,
                  log_entry, log_entry, candidate_id
              ))
              
              conn.commit()
              cur.close()
              conn.close()
              
              print(f"[DONE] Candidate {candidate_id} | {observation_verdict} | Score: {new_zombie_score} | Next: {next_observation_at.strftime('%H:%M')}")

    else:
      - id: no_candidates_due
        type: io.kestra.plugin.core.debug.Echo
        level: INFO
        format: "[POLL] No candidates due for observation"

  # ============================================================================
  # STAGE 4: Output Results & Reset Failure Counter
  # ============================================================================
  - id: reset_failure_counter
    type: io.kestra.plugin.scripts.python.Script
    description: Reset consecutive failure counter on successful run
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
    containerImage: python:3.11-slim
    beforeCommands:
      - pip install -q psycopg2-binary
    env:
      DATABASE_URL: "{{ secret('DATABASE_URL') }}"
    script: |
      import os
      import psycopg2
      
      db_url = os.environ.get('DATABASE_URL')
      
      conn = psycopg2.connect(db_url, sslmode='require')
      cur = conn.cursor()
      
      # Ensure workflow_health table exists
      cur.execute("""
          CREATE TABLE IF NOT EXISTS workflow_health (
              workflow_id VARCHAR(50) PRIMARY KEY,
              consecutive_failures INT DEFAULT 0,
              total_failures INT DEFAULT 0,
              last_failure_at TIMESTAMP WITH TIME ZONE,
              last_success_at TIMESTAMP WITH TIME ZONE,
              last_error TEXT,
              created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
              updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
          )
      """)
      
      # Reset consecutive failures on success (upsert to handle first run)
      cur.execute("""
          INSERT INTO workflow_health (workflow_id, consecutive_failures, last_success_at, updated_at)
          VALUES ('w2_observation_loop', 0, NOW(), NOW())
          ON CONFLICT (workflow_id) DO UPDATE SET
              consecutive_failures = 0,
              last_success_at = NOW(),
              updated_at = NOW()
      """)
      
      conn.commit()
      cur.close()
      conn.close()
      
      print("[W2] Success - reset failure counter")

  - id: output_poll_stats
    type: io.kestra.plugin.core.output.OutputValues
    values:
      poll_timestamp: "{{ trigger.date ?? execution.startDate }}"
      execution_id: "{{ execution.id }}"
      status: "completed"

errors:
  - id: observation_loop_failed
    type: io.kestra.plugin.core.flow.Sequential
    tasks:
      - id: log_w2_error
        type: io.kestra.plugin.core.log.Log
        message: "W2 Observation Loop encountered an error"
        level: ERROR
        
      - id: track_consecutive_failures
        type: io.kestra.plugin.scripts.python.Script
        description: Track consecutive failures and alert if threshold reached
        taskRunner:
          type: io.kestra.plugin.scripts.runner.docker.Docker
        containerImage: python:3.11-slim
        beforeCommands:
          - pip install -q psycopg2-binary
        env:
          DATABASE_URL: "{{ secret('DATABASE_URL') }}"
          EXECUTION_ID: "{{ execution.id }}"
        outputFiles:
          - failure_tracking.json
        script: |
          import os
          import json
          import psycopg2
          from datetime import datetime, timezone, timedelta
          
          db_url = os.environ.get('DATABASE_URL')
          execution_id = os.environ.get('EXECUTION_ID')
          now = datetime.now(timezone.utc)
          
          conn = psycopg2.connect(db_url, sslmode='require')
          cur = conn.cursor()
          
          # Check if w2_failure_tracking table exists, create if not
          cur.execute("""
              CREATE TABLE IF NOT EXISTS workflow_health (
                  workflow_id VARCHAR(50) PRIMARY KEY,
                  consecutive_failures INT DEFAULT 0,
                  total_failures INT DEFAULT 0,
                  last_failure_at TIMESTAMP WITH TIME ZONE,
                  last_success_at TIMESTAMP WITH TIME ZONE,
                  last_error TEXT,
                  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
              )
          """)
          
          # Get or create tracking record
          cur.execute("""
              INSERT INTO workflow_health (workflow_id, consecutive_failures, total_failures, last_failure_at, updated_at)
              VALUES ('w2_observation_loop', 1, 1, NOW(), NOW())
              ON CONFLICT (workflow_id) DO UPDATE SET
                  consecutive_failures = workflow_health.consecutive_failures + 1,
                  total_failures = workflow_health.total_failures + 1,
                  last_failure_at = NOW(),
                  updated_at = NOW()
              RETURNING consecutive_failures, total_failures
          """)
          
          result = cur.fetchone()
          consecutive = result[0]
          total = result[1]
          
          conn.commit()
          cur.close()
          conn.close()
          
          # Determine if alert needed (5+ consecutive failures)
          should_alert = consecutive >= 5
          
          output = {
              "consecutive_failures": consecutive,
              "total_failures": total,
              "should_alert": should_alert,
              "threshold": 5
          }
          
          with open('failure_tracking.json', 'w') as f:
              json.dump(output, f)
          
          print(f"[W2 FAILURE] Consecutive: {consecutive}, Total: {total}, Alert: {should_alert}")
          
      - id: check_alert_needed
        type: io.kestra.plugin.core.flow.If
        condition: "{{ outputs.track_consecutive_failures.outputFiles is defined and outputs.track_consecutive_failures.outputFiles['failure_tracking.json'] is defined and (read(outputs.track_consecutive_failures.outputFiles['failure_tracking.json']) | json).should_alert is defined and (read(outputs.track_consecutive_failures.outputFiles['failure_tracking.json']) | json).should_alert == true }}"
        then:
          - id: notify_admin_w2_failure
            type: io.kestra.plugin.notifications.mail.MailSend
            description: Alert admin about repeated W2 failures
            host: "{{ kv('SMTP_HOST') }}"
            port: "{{ kv('SMTP_PORT') }}"
            username: "{{ kv('SMTP_USERNAME') }}"
            password: "{{ kv('SMTP_PASSWORD') }}"
            from: "{{ kv('SMTP_FROM') }}"
            to:
              - "{{ kv('ADMIN_EMAIL') ?? kv('SMTP_FROM') }}"
            subject: "[ALERT] W2 Observation Loop - Repeated Failures"
            htmlTextContent: |
              <!DOCTYPE html>
              <html>
              <body style="margin: 0; padding: 20px; background: #0a0a0a; font-family: -apple-system, sans-serif;">
                <div style="max-width: 500px; margin: 0 auto; background: #111; border-radius: 12px; border: 1px solid #dc2626; padding: 24px;">
                  <h2 style="color: #dc2626; margin: 0 0 16px;">W2 Repeated Failures</h2>
                  <p style="color: #9ca3af; margin: 0 0 16px;">The observation loop has failed multiple times consecutively.</p>
                  <div style="background: #1a1a1a; padding: 12px; border-radius: 8px; margin-bottom: 12px;">
                    <p style="color: #6b7280; font-size: 11px; margin: 0 0 4px;">EXECUTION ID</p>
                    <p style="color: #f3f4f6; font-family: monospace; margin: 0;">{{ execution.id }}</p>
                  </div>
                  <div style="background: #1c1917; padding: 12px; border-radius: 8px; border-left: 3px solid #dc2626;">
                    <p style="color: #6b7280; font-size: 11px; margin: 0 0 4px;">STATUS</p>
                    <p style="color: #fca5a5; font-family: monospace; font-size: 12px; margin: 0;">Observation loop failed - check Kestra logs</p>
                  </div>
                  <p style="color: #9ca3af; font-size: 12px; margin: 16px 0;">Zombie observation is paused. Please investigate immediately.</p>
                  <p style="color: #4b5563; font-size: 11px; margin: 16px 0 0; text-align: center;">Services Doomsday - W2 Observation</p>
                </div>
              </body>
              </html>
              
      - id: output_failure
        type: io.kestra.plugin.core.output.OutputValues
        values:
          status: "failed"
          execution_id: "{{ execution.id }}"

triggers:
  - id: every_5_minutes
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "*/5 * * * *"
    recoverMissedSchedules: LAST
